{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "bGU6NwlsXFSt"
   },
   "outputs": [],
   "source": [
    "#@title Import Dependencies\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "_bNfVLRUYqZA"
   },
   "outputs": [],
   "source": [
    "#@title Define Hyperparameters\n",
    "\n",
    "input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
    "hidden_size = 500 # number of nodes at hidden layer\n",
    "num_classes = 10 # number of output classes discrete range [0,9]\n",
    "num_epochs = 50 # number of times which the entire dataset is passed throughout the model\n",
    "batch_size = 100 # the size of input data took for one iteration\n",
    "lr = 1e-2 # size of step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCsBCXMwbpH5"
   },
   "outputs": [],
   "source": [
    "#@title Downloading MNIST data\n",
    "\n",
    "train_data = dsets.MNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "test_data = dsets.MNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfDPBdnYgfGp"
   },
   "outputs": [],
   "source": [
    "#@title Loading the data\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                             batch_size = batch_size,\n",
    "                                             shuffle = True)\n",
    "\n",
    "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                      batch_size = batch_size, \n",
    "                                      shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class PhotonCountingP(nn.Module):\n",
    "    \"\"\" The probability of 1 photon in photon counting \n",
    "        (also the expectation value) with mean flux x \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PhotonCountingP, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1.-torch.exp(torch.abs(x)*-1.)\n",
    "    \n",
    "class BernoulliFunctionST(Function):\n",
    "    \"\"\" The 'Straight Through' stochastic Bernoulli activation\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "\n",
    "        return torch.bernoulli(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        return grad_output\n",
    "\n",
    "class PoissonFunctionST(Function):\n",
    "    \"\"\" The 'Straight Through' stochastic Poisson activation\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "\n",
    "        return torch.poisson(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        return grad_output\n",
    "    \n",
    "PoissonST = PoissonFunctionST.apply    \n",
    "BernoulliST = BernoulliFunctionST.apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fL-YXTvghaz_"
   },
   "outputs": [],
   "source": [
    "# #for photonActivation\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#   def __init__(self, input_size, hidden_size, num_classes, scale = 1, slope = 1, repeat = 5):\n",
    "#     super(Net,self).__init__()\n",
    "#     self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#     self.relu = nn.ReLU()\n",
    "#     self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "#     self.act = PhotonCountingP()\n",
    "#     self.sampler = BernoulliST\n",
    "#     self.scale = scale\n",
    "#     self.slope = slope\n",
    "#     self.repeat = repeat\n",
    "  \n",
    "#   def forward(self,x):\n",
    "#     out = self.fc1(x)\n",
    "#     # out = self.relu(out)\n",
    "#     probs = self.act(self.slope * out)\n",
    "#     out = self.sampler(probs.unsqueeze(0).repeat((self.repeat,)+(1,)*len(probs.shape))).mean(axis=0)*torch.sign(out)\n",
    "#     out = self.fc2(out)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for poisson\n",
    "from torch.distributions import Exponential\n",
    "\n",
    "def sample_poisson_relaxed(lmbd, num_samples=5, temperature = 0.01):\n",
    "    try:\n",
    "        sampler = torch.empty(num_samples)\n",
    "        z = Exponential(lmbd).rsample(sampler.size())\n",
    "        t = torch.cumsum(z, 0)\n",
    "        relaxed_indicator = torch.sigmoid((1.0 - t) / temperature)\n",
    "        N = torch.cumsum(relaxed_indicator, 0)\n",
    "        return N[-1]\n",
    "    except Exception:\n",
    "        return lmbd\n",
    "\n",
    "def bernoulli_sample(lmbd, num_samples = 1):\n",
    "  ones = torch.ones_like(lmbd)\n",
    "  negativelmbd = torch.mul(lmbd, -1)\n",
    "  probs = torch.sub(ones, torch.exp(negativelmbd)) \n",
    "  sample = torch.randn_like(lmbd) #add some way of taking an average of samples\n",
    "  out = torch.where(probs > sample, 1.0, 0.0) #will return 1 if probs > sample (ie. there is a shot detected from [1-e^-lmbd])\n",
    "  return out\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def bernoulli_reparam(lmbd):\n",
    "  # print(ones)\n",
    "  negativelmbd = torch.mul(lmbd, -1)\n",
    "  # print(negativelmbd)\n",
    "  zeroprobs = torch.exp(negativelmbd) #this will be e^-lmbd\n",
    "  # print(zeroprobs)\n",
    "  zeroprobs = torch.unsqueeze(zeroprobs, 2)\n",
    "  # print(zeroprobs)\n",
    "  ones = torch.ones_like(zeroprobs)\n",
    "  oneprobs = torch.sub(ones, zeroprobs) # 1-e^-lmbd\n",
    "  # print(oneprobs)\n",
    "  logits = torch.cat([oneprobs, zeroprobs], 2)\n",
    "  # print(logits)\n",
    "  sample = F.gumbel_softmax(logits, hard=False) #[a,b] where a=1 if there is a detection and b=1 if there is not\n",
    "  out = sample[:,:,0] #i think this will be 1 if there is a detection and 0 otherwise\n",
    "  # print(out)\n",
    "  return(out)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes, scale = 1, slope = 1, repeat = 5):\n",
    "    super(Net,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    # self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    self.repeat = repeat\n",
    "  \n",
    "  def forward(self,x):\n",
    "    out = self.fc1(x)\n",
    "    # out = sample_poisson_relaxed(torch.square(out), self.repeat)\n",
    "    # out = bernoulli_sample(sample_poisson_relaxed(torch.square(out), self.repeat))\n",
    "    out = bernoulli_reparam(sample_poisson_relaxed(out, self.repeat))\n",
    "    # out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3EPEqbjjfAT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda:0\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "#@title Build the model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(1)\n",
    "else:\n",
    "    print(0)\n",
    "print(torch.device(0))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u75Xa5VckuTH"
   },
   "outputs": [],
   "source": [
    "#@title Training the model\n",
    "\n",
    "\n",
    "def runModel(count):\n",
    "  accuracies = []\n",
    "  # photonCounts = []\n",
    "  hiddenLayer = []\n",
    "\n",
    "  # finalaccuracies = []\n",
    "  # finalCounts = []\n",
    "\n",
    "  for j in [50,100,200,300,400]:\n",
    "  # for j in [400]:\n",
    "\n",
    "    hidden_size = j\n",
    "    # count = j + 1\n",
    "    net = Net(input_size, hidden_size, num_classes, repeat= count)\n",
    "    if torch.cuda.is_available():\n",
    "       net.cuda()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam( net.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = 0.1)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.99, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "    max = 0\n",
    "    for epoch in range(num_epochs):\n",
    "      for i ,(images,labels) in enumerate(train_gen):\n",
    "        images = Variable(images.view(-1,28*28)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "          print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                    %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
    "        # if (i+1) % 100 == 0:\n",
    "        #   print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Accuracy: %.1f'\n",
    "        #             %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item(), accuracy))\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images,labels in test_gen:\n",
    "        images = Variable(images.view(-1,28*28)).cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        output = net(images)\n",
    "        _, predicted = torch.max(output,1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        total += labels.size(0)\n",
    "\n",
    "      accuracy = (100*correct)/(total+1)\n",
    "      if max < accuracy.item():\n",
    "        max = accuracy.item()\n",
    "\n",
    "    accuracies.append(max)\n",
    "    hiddenLayer.append(hidden_size)\n",
    "        # photonCounts.append(count)\n",
    "    # finalaccuracies.append(accuracy)\n",
    "    # finalCounts.append(count)\n",
    "  return accuracies, hiddenLayer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(accuracies)):\n",
    "#     accuracies[i] = accuracies[i].item()\n",
    "\n",
    "# for i in range(len(finalaccuracies)):\n",
    "#     finalaccuracies[i] = finalaccuracies[i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTPvMW5jHB9X"
   },
   "outputs": [],
   "source": [
    "#@title Evaluating the accuracy of the model\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for images,labels in test_gen:\n",
    "#   images = Variable(images.view(-1,28*28))\n",
    "#   labels = labels\n",
    "  \n",
    "#   output = net(images)\n",
    "#   _, predicted = torch.max(output,1)\n",
    "#   correct += (predicted == labels).sum()\n",
    "#   total += labels.size(0)\n",
    "\n",
    "# print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# print(photonCounts)\n",
    "# print(accuracies)\n",
    "# plt.plot(photonCounts, accuracies, 'o')\n",
    "# plt.plot(finalCounts, finalaccuracies)\n",
    "# plt.xlabel('Number of shots')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.title('Shot number vs. Accuracy after 10 epochs')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/600], Loss: 1.8664\n",
      "Epoch [1/50], Step [200/600], Loss: 1.2389\n",
      "Epoch [1/50], Step [300/600], Loss: 1.0732\n",
      "Epoch [1/50], Step [400/600], Loss: 0.9440\n",
      "Epoch [1/50], Step [500/600], Loss: 0.8164\n",
      "Epoch [1/50], Step [600/600], Loss: 0.6034\n",
      "Epoch [2/50], Step [100/600], Loss: 0.6323\n",
      "Epoch [2/50], Step [200/600], Loss: 0.5386\n",
      "Epoch [2/50], Step [300/600], Loss: 0.4378\n",
      "Epoch [2/50], Step [400/600], Loss: 0.3777\n",
      "Epoch [2/50], Step [500/600], Loss: 0.4116\n",
      "Epoch [2/50], Step [600/600], Loss: 0.5269\n",
      "Epoch [3/50], Step [100/600], Loss: 0.4387\n",
      "Epoch [3/50], Step [200/600], Loss: 0.4284\n",
      "Epoch [3/50], Step [300/600], Loss: 0.2849\n",
      "Epoch [3/50], Step [400/600], Loss: 0.4156\n",
      "Epoch [3/50], Step [500/600], Loss: 0.4735\n",
      "Epoch [3/50], Step [600/600], Loss: 0.4344\n",
      "Epoch [4/50], Step [100/600], Loss: 0.4747\n",
      "Epoch [4/50], Step [200/600], Loss: 0.3030\n",
      "Epoch [4/50], Step [300/600], Loss: 0.3561\n",
      "Epoch [4/50], Step [400/600], Loss: 0.4588\n",
      "Epoch [4/50], Step [500/600], Loss: 0.4791\n",
      "Epoch [4/50], Step [600/600], Loss: 0.3216\n",
      "Epoch [5/50], Step [100/600], Loss: 0.5492\n",
      "Epoch [5/50], Step [200/600], Loss: 0.3631\n",
      "Epoch [5/50], Step [300/600], Loss: 0.3475\n",
      "Epoch [5/50], Step [400/600], Loss: 0.2509\n",
      "Epoch [5/50], Step [500/600], Loss: 0.3104\n",
      "Epoch [5/50], Step [600/600], Loss: 0.2897\n",
      "Epoch [6/50], Step [100/600], Loss: 0.3663\n",
      "Epoch [6/50], Step [200/600], Loss: 0.3524\n",
      "Epoch [6/50], Step [300/600], Loss: 0.3639\n",
      "Epoch [6/50], Step [400/600], Loss: 0.1881\n",
      "Epoch [6/50], Step [500/600], Loss: 0.2554\n",
      "Epoch [6/50], Step [600/600], Loss: 0.2980\n",
      "Epoch [7/50], Step [100/600], Loss: 0.3304\n",
      "Epoch [7/50], Step [200/600], Loss: 0.3560\n",
      "Epoch [7/50], Step [300/600], Loss: 0.3816\n",
      "Epoch [7/50], Step [400/600], Loss: 0.3260\n",
      "Epoch [7/50], Step [500/600], Loss: 0.4267\n",
      "Epoch [7/50], Step [600/600], Loss: 0.3944\n",
      "Epoch [8/50], Step [100/600], Loss: 0.3766\n",
      "Epoch [8/50], Step [200/600], Loss: 0.1798\n",
      "Epoch [8/50], Step [300/600], Loss: 0.3549\n",
      "Epoch [8/50], Step [400/600], Loss: 0.3185\n",
      "Epoch [8/50], Step [500/600], Loss: 0.2306\n",
      "Epoch [8/50], Step [600/600], Loss: 0.3474\n",
      "Epoch [9/50], Step [100/600], Loss: 0.2170\n",
      "Epoch [9/50], Step [200/600], Loss: 0.3260\n",
      "Epoch [9/50], Step [300/600], Loss: 0.2915\n",
      "Epoch [9/50], Step [400/600], Loss: 0.2679\n",
      "Epoch [9/50], Step [500/600], Loss: 0.3191\n",
      "Epoch [9/50], Step [600/600], Loss: 0.2153\n",
      "Epoch [10/50], Step [100/600], Loss: 0.2828\n",
      "Epoch [10/50], Step [200/600], Loss: 0.2081\n",
      "Epoch [10/50], Step [300/600], Loss: 0.3097\n",
      "Epoch [10/50], Step [400/600], Loss: 0.2032\n",
      "Epoch [10/50], Step [500/600], Loss: 0.3822\n",
      "Epoch [10/50], Step [600/600], Loss: 0.2930\n",
      "Epoch [11/50], Step [100/600], Loss: 0.2026\n",
      "Epoch [11/50], Step [200/600], Loss: 0.2621\n",
      "Epoch [11/50], Step [300/600], Loss: 0.3175\n",
      "Epoch [11/50], Step [400/600], Loss: 0.2050\n",
      "Epoch [11/50], Step [500/600], Loss: 0.2780\n",
      "Epoch [11/50], Step [600/600], Loss: 0.1728\n",
      "Epoch [12/50], Step [100/600], Loss: 0.2923\n",
      "Epoch [12/50], Step [200/600], Loss: 0.2441\n",
      "Epoch [12/50], Step [300/600], Loss: 0.2640\n",
      "Epoch [12/50], Step [400/600], Loss: 0.1827\n",
      "Epoch [12/50], Step [500/600], Loss: 0.3376\n",
      "Epoch [12/50], Step [600/600], Loss: 0.2324\n",
      "Epoch [13/50], Step [100/600], Loss: 0.2257\n",
      "Epoch [13/50], Step [200/600], Loss: 0.3186\n",
      "Epoch [13/50], Step [300/600], Loss: 0.2546\n",
      "Epoch [13/50], Step [400/600], Loss: 0.3170\n",
      "Epoch [13/50], Step [500/600], Loss: 0.2682\n",
      "Epoch [13/50], Step [600/600], Loss: 0.2626\n",
      "Epoch [14/50], Step [100/600], Loss: 0.1273\n",
      "Epoch [14/50], Step [200/600], Loss: 0.2461\n",
      "Epoch [14/50], Step [300/600], Loss: 0.3096\n",
      "Epoch [14/50], Step [400/600], Loss: 0.2858\n",
      "Epoch [14/50], Step [500/600], Loss: 0.1431\n",
      "Epoch [14/50], Step [600/600], Loss: 0.2195\n",
      "Epoch [15/50], Step [100/600], Loss: 0.2230\n",
      "Epoch [15/50], Step [200/600], Loss: 0.3098\n",
      "Epoch [15/50], Step [300/600], Loss: 0.2730\n",
      "Epoch [15/50], Step [400/600], Loss: 0.2002\n",
      "Epoch [15/50], Step [500/600], Loss: 0.1645\n",
      "Epoch [15/50], Step [600/600], Loss: 0.2163\n",
      "Epoch [16/50], Step [100/600], Loss: 0.3128\n",
      "Epoch [16/50], Step [200/600], Loss: 0.2714\n",
      "Epoch [16/50], Step [300/600], Loss: 0.1393\n",
      "Epoch [16/50], Step [400/600], Loss: 0.2913\n",
      "Epoch [16/50], Step [500/600], Loss: 0.2980\n",
      "Epoch [16/50], Step [600/600], Loss: 0.2973\n",
      "Epoch [17/50], Step [100/600], Loss: 0.2089\n",
      "Epoch [17/50], Step [200/600], Loss: 0.3174\n",
      "Epoch [17/50], Step [300/600], Loss: 0.2889\n",
      "Epoch [17/50], Step [400/600], Loss: 0.4333\n",
      "Epoch [17/50], Step [500/600], Loss: 0.3046\n",
      "Epoch [17/50], Step [600/600], Loss: 0.1635\n",
      "Epoch [18/50], Step [100/600], Loss: 0.2976\n",
      "Epoch [18/50], Step [200/600], Loss: 0.2187\n",
      "Epoch [18/50], Step [300/600], Loss: 0.1230\n",
      "Epoch [18/50], Step [400/600], Loss: 0.2245\n",
      "Epoch [18/50], Step [500/600], Loss: 0.2269\n",
      "Epoch [18/50], Step [600/600], Loss: 0.2124\n",
      "Epoch [19/50], Step [100/600], Loss: 0.2442\n",
      "Epoch [19/50], Step [200/600], Loss: 0.3128\n",
      "Epoch [19/50], Step [300/600], Loss: 0.1511\n",
      "Epoch [19/50], Step [400/600], Loss: 0.1400\n",
      "Epoch [19/50], Step [500/600], Loss: 0.3884\n",
      "Epoch [19/50], Step [600/600], Loss: 0.1928\n",
      "Epoch [20/50], Step [100/600], Loss: 0.2156\n",
      "Epoch [20/50], Step [200/600], Loss: 0.1249\n",
      "Epoch [20/50], Step [300/600], Loss: 0.2536\n",
      "Epoch [20/50], Step [400/600], Loss: 0.2214\n",
      "Epoch [20/50], Step [500/600], Loss: 0.1758\n",
      "Epoch [20/50], Step [600/600], Loss: 0.2680\n",
      "Epoch [21/50], Step [100/600], Loss: 0.1996\n",
      "Epoch [21/50], Step [200/600], Loss: 0.2797\n",
      "Epoch [21/50], Step [300/600], Loss: 0.1097\n",
      "Epoch [21/50], Step [400/600], Loss: 0.1152\n",
      "Epoch [21/50], Step [500/600], Loss: 0.1119\n",
      "Epoch [21/50], Step [600/600], Loss: 0.2314\n",
      "Epoch [22/50], Step [100/600], Loss: 0.1303\n",
      "Epoch [22/50], Step [200/600], Loss: 0.1959\n",
      "Epoch [22/50], Step [300/600], Loss: 0.2183\n",
      "Epoch [22/50], Step [400/600], Loss: 0.1547\n",
      "Epoch [22/50], Step [500/600], Loss: 0.2967\n",
      "Epoch [22/50], Step [600/600], Loss: 0.2126\n",
      "Epoch [23/50], Step [100/600], Loss: 0.3029\n",
      "Epoch [23/50], Step [200/600], Loss: 0.2582\n",
      "Epoch [23/50], Step [300/600], Loss: 0.2557\n",
      "Epoch [23/50], Step [400/600], Loss: 0.1655\n",
      "Epoch [23/50], Step [500/600], Loss: 0.2174\n",
      "Epoch [23/50], Step [600/600], Loss: 0.1942\n",
      "Epoch [24/50], Step [100/600], Loss: 0.2425\n",
      "Epoch [24/50], Step [200/600], Loss: 0.2194\n",
      "Epoch [24/50], Step [300/600], Loss: 0.1368\n",
      "Epoch [24/50], Step [400/600], Loss: 0.2046\n",
      "Epoch [24/50], Step [500/600], Loss: 0.1661\n",
      "Epoch [24/50], Step [600/600], Loss: 0.1501\n",
      "Epoch [25/50], Step [100/600], Loss: 0.1893\n",
      "Epoch [25/50], Step [200/600], Loss: 0.2208\n",
      "Epoch [25/50], Step [300/600], Loss: 0.3493\n",
      "Epoch [25/50], Step [400/600], Loss: 0.1818\n",
      "Epoch [25/50], Step [500/600], Loss: 0.2565\n",
      "Epoch [25/50], Step [600/600], Loss: 0.2137\n",
      "Epoch [26/50], Step [100/600], Loss: 0.3015\n",
      "Epoch [26/50], Step [200/600], Loss: 0.2050\n",
      "Epoch [26/50], Step [300/600], Loss: 0.2092\n",
      "Epoch [26/50], Step [400/600], Loss: 0.2149\n",
      "Epoch [26/50], Step [500/600], Loss: 0.2571\n",
      "Epoch [26/50], Step [600/600], Loss: 0.2303\n",
      "Epoch [27/50], Step [100/600], Loss: 0.1609\n",
      "Epoch [27/50], Step [200/600], Loss: 0.1604\n",
      "Epoch [27/50], Step [300/600], Loss: 0.3264\n",
      "Epoch [27/50], Step [400/600], Loss: 0.2845\n",
      "Epoch [27/50], Step [500/600], Loss: 0.1521\n",
      "Epoch [27/50], Step [600/600], Loss: 0.3956\n",
      "Epoch [28/50], Step [100/600], Loss: 0.2735\n",
      "Epoch [28/50], Step [200/600], Loss: 0.1728\n",
      "Epoch [28/50], Step [300/600], Loss: 0.2228\n",
      "Epoch [28/50], Step [400/600], Loss: 0.1522\n",
      "Epoch [28/50], Step [500/600], Loss: 0.3352\n",
      "Epoch [28/50], Step [600/600], Loss: 0.1485\n",
      "Epoch [29/50], Step [100/600], Loss: 0.1765\n",
      "Epoch [29/50], Step [200/600], Loss: 0.1480\n",
      "Epoch [29/50], Step [300/600], Loss: 0.1233\n",
      "Epoch [29/50], Step [400/600], Loss: 0.0758\n",
      "Epoch [29/50], Step [500/600], Loss: 0.1544\n",
      "Epoch [29/50], Step [600/600], Loss: 0.1566\n",
      "Epoch [30/50], Step [100/600], Loss: 0.1812\n",
      "Epoch [30/50], Step [200/600], Loss: 0.1149\n",
      "Epoch [30/50], Step [300/600], Loss: 0.2173\n",
      "Epoch [30/50], Step [400/600], Loss: 0.1360\n",
      "Epoch [30/50], Step [500/600], Loss: 0.1846\n",
      "Epoch [30/50], Step [600/600], Loss: 0.2132\n",
      "Epoch [31/50], Step [100/600], Loss: 0.2470\n",
      "Epoch [31/50], Step [200/600], Loss: 0.1805\n",
      "Epoch [31/50], Step [300/600], Loss: 0.2255\n",
      "Epoch [31/50], Step [400/600], Loss: 0.1623\n",
      "Epoch [31/50], Step [500/600], Loss: 0.2697\n",
      "Epoch [31/50], Step [600/600], Loss: 0.1679\n",
      "Epoch [32/50], Step [100/600], Loss: 0.2517\n",
      "Epoch [32/50], Step [200/600], Loss: 0.1704\n",
      "Epoch [32/50], Step [300/600], Loss: 0.1463\n",
      "Epoch [32/50], Step [400/600], Loss: 0.1001\n",
      "Epoch [32/50], Step [500/600], Loss: 0.2013\n",
      "Epoch [32/50], Step [600/600], Loss: 0.1616\n",
      "Epoch [33/50], Step [100/600], Loss: 0.2077\n",
      "Epoch [33/50], Step [200/600], Loss: 0.1977\n",
      "Epoch [33/50], Step [300/600], Loss: 0.1587\n",
      "Epoch [33/50], Step [400/600], Loss: 0.1029\n",
      "Epoch [33/50], Step [500/600], Loss: 0.1367\n",
      "Epoch [33/50], Step [600/600], Loss: 0.1246\n",
      "Epoch [34/50], Step [100/600], Loss: 0.1766\n",
      "Epoch [34/50], Step [200/600], Loss: 0.1091\n",
      "Epoch [34/50], Step [300/600], Loss: 0.1119\n",
      "Epoch [34/50], Step [400/600], Loss: 0.1315\n",
      "Epoch [34/50], Step [500/600], Loss: 0.1772\n",
      "Epoch [34/50], Step [600/600], Loss: 0.0801\n",
      "Epoch [35/50], Step [100/600], Loss: 0.2841\n",
      "Epoch [35/50], Step [200/600], Loss: 0.1504\n",
      "Epoch [35/50], Step [300/600], Loss: 0.2353\n",
      "Epoch [35/50], Step [400/600], Loss: 0.2279\n",
      "Epoch [35/50], Step [500/600], Loss: 0.1654\n",
      "Epoch [35/50], Step [600/600], Loss: 0.1445\n",
      "Epoch [36/50], Step [100/600], Loss: 0.1085\n",
      "Epoch [36/50], Step [200/600], Loss: 0.1530\n",
      "Epoch [36/50], Step [300/600], Loss: 0.3028\n",
      "Epoch [36/50], Step [400/600], Loss: 0.1260\n",
      "Epoch [36/50], Step [500/600], Loss: 0.2218\n",
      "Epoch [36/50], Step [600/600], Loss: 0.2242\n",
      "Epoch [37/50], Step [100/600], Loss: 0.0989\n",
      "Epoch [37/50], Step [200/600], Loss: 0.1536\n",
      "Epoch [37/50], Step [300/600], Loss: 0.1565\n",
      "Epoch [37/50], Step [400/600], Loss: 0.2309\n",
      "Epoch [37/50], Step [500/600], Loss: 0.0985\n",
      "Epoch [37/50], Step [600/600], Loss: 0.2367\n",
      "Epoch [38/50], Step [100/600], Loss: 0.1370\n",
      "Epoch [38/50], Step [200/600], Loss: 0.1877\n",
      "Epoch [38/50], Step [300/600], Loss: 0.1625\n",
      "Epoch [38/50], Step [400/600], Loss: 0.1180\n",
      "Epoch [38/50], Step [500/600], Loss: 0.1886\n",
      "Epoch [38/50], Step [600/600], Loss: 0.1592\n",
      "Epoch [39/50], Step [100/600], Loss: 0.1142\n",
      "Epoch [39/50], Step [200/600], Loss: 0.1071\n",
      "Epoch [39/50], Step [300/600], Loss: 0.1686\n",
      "Epoch [39/50], Step [400/600], Loss: 0.1568\n",
      "Epoch [39/50], Step [500/600], Loss: 0.1464\n",
      "Epoch [39/50], Step [600/600], Loss: 0.2167\n",
      "Epoch [40/50], Step [100/600], Loss: 0.1731\n",
      "Epoch [40/50], Step [200/600], Loss: 0.2409\n",
      "Epoch [40/50], Step [300/600], Loss: 0.1656\n",
      "Epoch [40/50], Step [400/600], Loss: 0.2115\n",
      "Epoch [40/50], Step [500/600], Loss: 0.1762\n",
      "Epoch [40/50], Step [600/600], Loss: 0.2132\n",
      "Epoch [41/50], Step [100/600], Loss: 0.1663\n",
      "Epoch [41/50], Step [200/600], Loss: 0.1375\n",
      "Epoch [41/50], Step [300/600], Loss: 0.1698\n",
      "Epoch [41/50], Step [400/600], Loss: 0.1489\n",
      "Epoch [41/50], Step [500/600], Loss: 0.1179\n",
      "Epoch [41/50], Step [600/600], Loss: 0.1810\n",
      "Epoch [42/50], Step [100/600], Loss: 0.2417\n",
      "Epoch [42/50], Step [200/600], Loss: 0.1522\n",
      "Epoch [42/50], Step [300/600], Loss: 0.1373\n",
      "Epoch [42/50], Step [400/600], Loss: 0.1621\n",
      "Epoch [42/50], Step [500/600], Loss: 0.2331\n",
      "Epoch [42/50], Step [600/600], Loss: 0.1216\n",
      "Epoch [43/50], Step [100/600], Loss: 0.1812\n",
      "Epoch [43/50], Step [200/600], Loss: 0.1748\n",
      "Epoch [43/50], Step [300/600], Loss: 0.1186\n",
      "Epoch [43/50], Step [400/600], Loss: 0.1921\n",
      "Epoch [43/50], Step [500/600], Loss: 0.1652\n",
      "Epoch [43/50], Step [600/600], Loss: 0.1834\n",
      "Epoch [44/50], Step [100/600], Loss: 0.1408\n",
      "Epoch [44/50], Step [200/600], Loss: 0.0829\n",
      "Epoch [44/50], Step [300/600], Loss: 0.1537\n",
      "Epoch [44/50], Step [400/600], Loss: 0.1926\n",
      "Epoch [44/50], Step [500/600], Loss: 0.2176\n",
      "Epoch [44/50], Step [600/600], Loss: 0.2203\n",
      "Epoch [45/50], Step [100/600], Loss: 0.1238\n",
      "Epoch [45/50], Step [200/600], Loss: 0.1575\n",
      "Epoch [45/50], Step [300/600], Loss: 0.1597\n",
      "Epoch [45/50], Step [400/600], Loss: 0.1205\n",
      "Epoch [45/50], Step [500/600], Loss: 0.1718\n",
      "Epoch [45/50], Step [600/600], Loss: 0.0863\n",
      "Epoch [46/50], Step [100/600], Loss: 0.1589\n",
      "Epoch [46/50], Step [200/600], Loss: 0.1891\n",
      "Epoch [46/50], Step [300/600], Loss: 0.1534\n",
      "Epoch [46/50], Step [400/600], Loss: 0.1227\n",
      "Epoch [46/50], Step [500/600], Loss: 0.1166\n",
      "Epoch [46/50], Step [600/600], Loss: 0.1861\n",
      "Epoch [47/50], Step [100/600], Loss: 0.1988\n",
      "Epoch [47/50], Step [200/600], Loss: 0.2934\n",
      "Epoch [47/50], Step [300/600], Loss: 0.1790\n",
      "Epoch [47/50], Step [400/600], Loss: 0.1025\n",
      "Epoch [47/50], Step [500/600], Loss: 0.0685\n",
      "Epoch [47/50], Step [600/600], Loss: 0.1107\n",
      "Epoch [48/50], Step [100/600], Loss: 0.0931\n",
      "Epoch [48/50], Step [200/600], Loss: 0.1362\n",
      "Epoch [48/50], Step [300/600], Loss: 0.1246\n",
      "Epoch [48/50], Step [400/600], Loss: 0.1102\n",
      "Epoch [48/50], Step [500/600], Loss: 0.1239\n",
      "Epoch [48/50], Step [600/600], Loss: 0.2278\n",
      "Epoch [49/50], Step [100/600], Loss: 0.0726\n",
      "Epoch [49/50], Step [200/600], Loss: 0.1726\n",
      "Epoch [49/50], Step [300/600], Loss: 0.1187\n",
      "Epoch [49/50], Step [400/600], Loss: 0.1655\n",
      "Epoch [49/50], Step [500/600], Loss: 0.1121\n",
      "Epoch [49/50], Step [600/600], Loss: 0.1276\n",
      "Epoch [50/50], Step [100/600], Loss: 0.2213\n",
      "Epoch [50/50], Step [200/600], Loss: 0.1759\n",
      "Epoch [50/50], Step [300/600], Loss: 0.2898\n",
      "Epoch [50/50], Step [400/600], Loss: 0.2224\n",
      "Epoch [50/50], Step [500/600], Loss: 0.1238\n",
      "Epoch [50/50], Step [600/600], Loss: 0.1565\n",
      "Epoch [1/50], Step [100/600], Loss: 1.7957\n",
      "Epoch [1/50], Step [200/600], Loss: 1.1988\n",
      "Epoch [1/50], Step [300/600], Loss: 0.7956\n",
      "Epoch [1/50], Step [400/600], Loss: 0.5933\n",
      "Epoch [1/50], Step [500/600], Loss: 0.5572\n",
      "Epoch [1/50], Step [600/600], Loss: 0.5973\n",
      "Epoch [2/50], Step [100/600], Loss: 0.5732\n",
      "Epoch [2/50], Step [200/600], Loss: 0.4923\n",
      "Epoch [2/50], Step [300/600], Loss: 0.5237\n",
      "Epoch [2/50], Step [400/600], Loss: 0.3491\n",
      "Epoch [2/50], Step [500/600], Loss: 0.3523\n",
      "Epoch [2/50], Step [600/600], Loss: 0.2884\n",
      "Epoch [3/50], Step [100/600], Loss: 0.3157\n",
      "Epoch [3/50], Step [200/600], Loss: 0.3445\n",
      "Epoch [3/50], Step [300/600], Loss: 0.3696\n",
      "Epoch [3/50], Step [400/600], Loss: 0.3774\n",
      "Epoch [3/50], Step [500/600], Loss: 0.3705\n",
      "Epoch [3/50], Step [600/600], Loss: 0.2056\n",
      "Epoch [4/50], Step [100/600], Loss: 0.2903\n",
      "Epoch [4/50], Step [200/600], Loss: 0.2358\n",
      "Epoch [4/50], Step [300/600], Loss: 0.5421\n",
      "Epoch [4/50], Step [400/600], Loss: 0.4061\n",
      "Epoch [4/50], Step [500/600], Loss: 0.3519\n",
      "Epoch [4/50], Step [600/600], Loss: 0.4020\n",
      "Epoch [5/50], Step [100/600], Loss: 0.2592\n",
      "Epoch [5/50], Step [200/600], Loss: 0.2663\n",
      "Epoch [5/50], Step [300/600], Loss: 0.3396\n",
      "Epoch [5/50], Step [400/600], Loss: 0.2951\n",
      "Epoch [5/50], Step [500/600], Loss: 0.2663\n",
      "Epoch [5/50], Step [600/600], Loss: 0.2053\n",
      "Epoch [6/50], Step [100/600], Loss: 0.4435\n",
      "Epoch [6/50], Step [200/600], Loss: 0.2687\n",
      "Epoch [6/50], Step [300/600], Loss: 0.3520\n",
      "Epoch [6/50], Step [400/600], Loss: 0.1625\n",
      "Epoch [6/50], Step [500/600], Loss: 0.2346\n",
      "Epoch [6/50], Step [600/600], Loss: 0.2267\n",
      "Epoch [7/50], Step [100/600], Loss: 0.2716\n",
      "Epoch [7/50], Step [200/600], Loss: 0.3175\n",
      "Epoch [7/50], Step [300/600], Loss: 0.2095\n",
      "Epoch [7/50], Step [400/600], Loss: 0.2662\n",
      "Epoch [7/50], Step [500/600], Loss: 0.2464\n",
      "Epoch [7/50], Step [600/600], Loss: 0.2542\n",
      "Epoch [8/50], Step [100/600], Loss: 0.2754\n",
      "Epoch [8/50], Step [200/600], Loss: 0.3650\n",
      "Epoch [8/50], Step [300/600], Loss: 0.2050\n",
      "Epoch [8/50], Step [400/600], Loss: 0.2491\n",
      "Epoch [8/50], Step [500/600], Loss: 0.3540\n",
      "Epoch [8/50], Step [600/600], Loss: 0.1594\n",
      "Epoch [9/50], Step [100/600], Loss: 0.2346\n",
      "Epoch [9/50], Step [200/600], Loss: 0.1754\n",
      "Epoch [9/50], Step [300/600], Loss: 0.3207\n",
      "Epoch [9/50], Step [400/600], Loss: 0.2506\n",
      "Epoch [9/50], Step [500/600], Loss: 0.1621\n",
      "Epoch [9/50], Step [600/600], Loss: 0.1800\n",
      "Epoch [10/50], Step [100/600], Loss: 0.2971\n",
      "Epoch [10/50], Step [200/600], Loss: 0.2578\n",
      "Epoch [10/50], Step [300/600], Loss: 0.3528\n",
      "Epoch [10/50], Step [400/600], Loss: 0.2966\n",
      "Epoch [10/50], Step [500/600], Loss: 0.1987\n",
      "Epoch [10/50], Step [600/600], Loss: 0.4190\n",
      "Epoch [11/50], Step [100/600], Loss: 0.2511\n",
      "Epoch [11/50], Step [200/600], Loss: 0.2190\n",
      "Epoch [11/50], Step [300/600], Loss: 0.2391\n",
      "Epoch [11/50], Step [400/600], Loss: 0.1377\n",
      "Epoch [11/50], Step [500/600], Loss: 0.1617\n",
      "Epoch [11/50], Step [600/600], Loss: 0.0790\n",
      "Epoch [12/50], Step [100/600], Loss: 0.1878\n",
      "Epoch [12/50], Step [200/600], Loss: 0.1866\n",
      "Epoch [12/50], Step [300/600], Loss: 0.2277\n",
      "Epoch [12/50], Step [400/600], Loss: 0.2354\n",
      "Epoch [12/50], Step [500/600], Loss: 0.2863\n",
      "Epoch [12/50], Step [600/600], Loss: 0.2534\n",
      "Epoch [13/50], Step [100/600], Loss: 0.3727\n",
      "Epoch [13/50], Step [200/600], Loss: 0.2946\n",
      "Epoch [13/50], Step [300/600], Loss: 0.4439\n",
      "Epoch [13/50], Step [400/600], Loss: 0.4408\n",
      "Epoch [13/50], Step [500/600], Loss: 0.2885\n",
      "Epoch [13/50], Step [600/600], Loss: 0.2640\n",
      "Epoch [14/50], Step [100/600], Loss: 0.1874\n",
      "Epoch [14/50], Step [200/600], Loss: 0.3175\n",
      "Epoch [14/50], Step [300/600], Loss: 0.2270\n",
      "Epoch [14/50], Step [400/600], Loss: 0.1898\n",
      "Epoch [14/50], Step [500/600], Loss: 0.3096\n",
      "Epoch [14/50], Step [600/600], Loss: 0.2542\n",
      "Epoch [15/50], Step [100/600], Loss: 0.2109\n",
      "Epoch [15/50], Step [200/600], Loss: 0.2995\n",
      "Epoch [15/50], Step [300/600], Loss: 0.2383\n",
      "Epoch [15/50], Step [400/600], Loss: 0.1017\n",
      "Epoch [15/50], Step [500/600], Loss: 0.0978\n",
      "Epoch [15/50], Step [600/600], Loss: 0.1298\n",
      "Epoch [16/50], Step [100/600], Loss: 0.1770\n",
      "Epoch [16/50], Step [200/600], Loss: 0.2341\n",
      "Epoch [16/50], Step [300/600], Loss: 0.2328\n",
      "Epoch [16/50], Step [400/600], Loss: 0.1531\n",
      "Epoch [16/50], Step [500/600], Loss: 0.2216\n",
      "Epoch [16/50], Step [600/600], Loss: 0.2222\n",
      "Epoch [17/50], Step [100/600], Loss: 0.1040\n",
      "Epoch [17/50], Step [200/600], Loss: 0.1091\n",
      "Epoch [17/50], Step [300/600], Loss: 0.1374\n",
      "Epoch [17/50], Step [400/600], Loss: 0.2749\n",
      "Epoch [17/50], Step [500/600], Loss: 0.2378\n",
      "Epoch [17/50], Step [600/600], Loss: 0.1517\n",
      "Epoch [18/50], Step [100/600], Loss: 0.1928\n",
      "Epoch [18/50], Step [200/600], Loss: 0.3444\n",
      "Epoch [18/50], Step [300/600], Loss: 0.1365\n",
      "Epoch [18/50], Step [400/600], Loss: 0.0746\n",
      "Epoch [18/50], Step [500/600], Loss: 0.1527\n",
      "Epoch [18/50], Step [600/600], Loss: 0.1478\n",
      "Epoch [19/50], Step [100/600], Loss: 0.1259\n",
      "Epoch [19/50], Step [200/600], Loss: 0.2521\n",
      "Epoch [19/50], Step [300/600], Loss: 0.1504\n",
      "Epoch [19/50], Step [400/600], Loss: 0.1790\n",
      "Epoch [19/50], Step [500/600], Loss: 0.2310\n",
      "Epoch [19/50], Step [600/600], Loss: 0.0738\n",
      "Epoch [20/50], Step [100/600], Loss: 0.1208\n",
      "Epoch [20/50], Step [200/600], Loss: 0.1327\n",
      "Epoch [20/50], Step [300/600], Loss: 0.1868\n",
      "Epoch [20/50], Step [400/600], Loss: 0.1625\n",
      "Epoch [20/50], Step [500/600], Loss: 0.2230\n",
      "Epoch [20/50], Step [600/600], Loss: 0.1498\n",
      "Epoch [21/50], Step [100/600], Loss: 0.2460\n",
      "Epoch [21/50], Step [200/600], Loss: 0.2269\n",
      "Epoch [21/50], Step [300/600], Loss: 0.1523\n",
      "Epoch [21/50], Step [400/600], Loss: 0.0850\n",
      "Epoch [21/50], Step [500/600], Loss: 0.3454\n",
      "Epoch [21/50], Step [600/600], Loss: 0.3768\n",
      "Epoch [22/50], Step [100/600], Loss: 0.1011\n",
      "Epoch [22/50], Step [200/600], Loss: 0.1584\n",
      "Epoch [22/50], Step [300/600], Loss: 0.2256\n",
      "Epoch [22/50], Step [400/600], Loss: 0.1100\n",
      "Epoch [22/50], Step [500/600], Loss: 0.2131\n",
      "Epoch [22/50], Step [600/600], Loss: 0.2786\n",
      "Epoch [23/50], Step [100/600], Loss: 0.1125\n",
      "Epoch [23/50], Step [200/600], Loss: 0.2444\n",
      "Epoch [23/50], Step [300/600], Loss: 0.2162\n",
      "Epoch [23/50], Step [400/600], Loss: 0.1430\n",
      "Epoch [23/50], Step [500/600], Loss: 0.1675\n",
      "Epoch [23/50], Step [600/600], Loss: 0.1445\n",
      "Epoch [24/50], Step [100/600], Loss: 0.1997\n",
      "Epoch [24/50], Step [200/600], Loss: 0.1738\n",
      "Epoch [24/50], Step [300/600], Loss: 0.1042\n",
      "Epoch [24/50], Step [400/600], Loss: 0.1053\n",
      "Epoch [24/50], Step [500/600], Loss: 0.1072\n",
      "Epoch [24/50], Step [600/600], Loss: 0.2146\n",
      "Epoch [25/50], Step [100/600], Loss: 0.1129\n",
      "Epoch [25/50], Step [200/600], Loss: 0.2011\n",
      "Epoch [25/50], Step [300/600], Loss: 0.2076\n",
      "Epoch [25/50], Step [400/600], Loss: 0.2644\n",
      "Epoch [25/50], Step [500/600], Loss: 0.1296\n",
      "Epoch [25/50], Step [600/600], Loss: 0.1148\n",
      "Epoch [26/50], Step [100/600], Loss: 0.0887\n",
      "Epoch [26/50], Step [200/600], Loss: 0.1804\n",
      "Epoch [26/50], Step [300/600], Loss: 0.1199\n",
      "Epoch [26/50], Step [400/600], Loss: 0.1246\n",
      "Epoch [26/50], Step [500/600], Loss: 0.0806\n",
      "Epoch [26/50], Step [600/600], Loss: 0.1209\n",
      "Epoch [27/50], Step [100/600], Loss: 0.1684\n",
      "Epoch [27/50], Step [200/600], Loss: 0.0770\n",
      "Epoch [27/50], Step [300/600], Loss: 0.0830\n",
      "Epoch [27/50], Step [400/600], Loss: 0.1028\n",
      "Epoch [27/50], Step [500/600], Loss: 0.1589\n",
      "Epoch [27/50], Step [600/600], Loss: 0.1383\n",
      "Epoch [28/50], Step [100/600], Loss: 0.1543\n",
      "Epoch [28/50], Step [200/600], Loss: 0.0430\n",
      "Epoch [28/50], Step [300/600], Loss: 0.1330\n",
      "Epoch [28/50], Step [400/600], Loss: 0.1547\n",
      "Epoch [28/50], Step [500/600], Loss: 0.1211\n",
      "Epoch [28/50], Step [600/600], Loss: 0.1344\n",
      "Epoch [29/50], Step [100/600], Loss: 0.1058\n",
      "Epoch [29/50], Step [200/600], Loss: 0.0656\n",
      "Epoch [29/50], Step [300/600], Loss: 0.1836\n",
      "Epoch [29/50], Step [400/600], Loss: 0.1013\n",
      "Epoch [29/50], Step [500/600], Loss: 0.1144\n",
      "Epoch [29/50], Step [600/600], Loss: 0.0796\n",
      "Epoch [30/50], Step [100/600], Loss: 0.1001\n",
      "Epoch [30/50], Step [200/600], Loss: 0.1472\n",
      "Epoch [30/50], Step [300/600], Loss: 0.1593\n",
      "Epoch [30/50], Step [400/600], Loss: 0.1422\n",
      "Epoch [30/50], Step [500/600], Loss: 0.2965\n",
      "Epoch [30/50], Step [600/600], Loss: 0.1812\n",
      "Epoch [31/50], Step [100/600], Loss: 0.1842\n",
      "Epoch [31/50], Step [200/600], Loss: 0.1291\n",
      "Epoch [31/50], Step [300/600], Loss: 0.1314\n",
      "Epoch [31/50], Step [400/600], Loss: 0.1203\n",
      "Epoch [31/50], Step [500/600], Loss: 0.1152\n",
      "Epoch [31/50], Step [600/600], Loss: 0.1159\n",
      "Epoch [32/50], Step [100/600], Loss: 0.1664\n",
      "Epoch [32/50], Step [200/600], Loss: 0.0985\n",
      "Epoch [32/50], Step [300/600], Loss: 0.0902\n",
      "Epoch [32/50], Step [400/600], Loss: 0.0972\n",
      "Epoch [32/50], Step [500/600], Loss: 0.0872\n",
      "Epoch [32/50], Step [600/600], Loss: 0.2237\n",
      "Epoch [33/50], Step [100/600], Loss: 0.2283\n",
      "Epoch [33/50], Step [200/600], Loss: 0.0743\n",
      "Epoch [33/50], Step [300/600], Loss: 0.0882\n",
      "Epoch [33/50], Step [400/600], Loss: 0.1064\n",
      "Epoch [33/50], Step [500/600], Loss: 0.0819\n",
      "Epoch [33/50], Step [600/600], Loss: 0.1310\n",
      "Epoch [34/50], Step [100/600], Loss: 0.1596\n",
      "Epoch [34/50], Step [200/600], Loss: 0.1584\n",
      "Epoch [34/50], Step [300/600], Loss: 0.1616\n",
      "Epoch [34/50], Step [400/600], Loss: 0.2686\n",
      "Epoch [34/50], Step [500/600], Loss: 0.1276\n",
      "Epoch [34/50], Step [600/600], Loss: 0.1263\n",
      "Epoch [35/50], Step [100/600], Loss: 0.0885\n",
      "Epoch [35/50], Step [200/600], Loss: 0.1113\n",
      "Epoch [35/50], Step [300/600], Loss: 0.1598\n",
      "Epoch [35/50], Step [400/600], Loss: 0.1432\n",
      "Epoch [35/50], Step [500/600], Loss: 0.2873\n",
      "Epoch [35/50], Step [600/600], Loss: 0.0881\n",
      "Epoch [36/50], Step [100/600], Loss: 0.1237\n",
      "Epoch [36/50], Step [200/600], Loss: 0.1675\n",
      "Epoch [36/50], Step [300/600], Loss: 0.2708\n",
      "Epoch [36/50], Step [400/600], Loss: 0.1502\n",
      "Epoch [36/50], Step [500/600], Loss: 0.0928\n",
      "Epoch [36/50], Step [600/600], Loss: 0.1458\n",
      "Epoch [37/50], Step [100/600], Loss: 0.1702\n",
      "Epoch [37/50], Step [200/600], Loss: 0.2164\n",
      "Epoch [37/50], Step [300/600], Loss: 0.0614\n",
      "Epoch [37/50], Step [400/600], Loss: 0.1033\n",
      "Epoch [37/50], Step [500/600], Loss: 0.1882\n",
      "Epoch [37/50], Step [600/600], Loss: 0.1257\n",
      "Epoch [38/50], Step [100/600], Loss: 0.2708\n",
      "Epoch [38/50], Step [200/600], Loss: 0.1952\n",
      "Epoch [38/50], Step [300/600], Loss: 0.1938\n",
      "Epoch [38/50], Step [400/600], Loss: 0.1369\n",
      "Epoch [38/50], Step [500/600], Loss: 0.2543\n",
      "Epoch [38/50], Step [600/600], Loss: 0.0794\n",
      "Epoch [39/50], Step [100/600], Loss: 0.0775\n",
      "Epoch [39/50], Step [200/600], Loss: 0.1237\n",
      "Epoch [39/50], Step [300/600], Loss: 0.0541\n",
      "Epoch [39/50], Step [400/600], Loss: 0.1219\n",
      "Epoch [39/50], Step [500/600], Loss: 0.1010\n",
      "Epoch [39/50], Step [600/600], Loss: 0.1194\n",
      "Epoch [40/50], Step [100/600], Loss: 0.1371\n",
      "Epoch [40/50], Step [200/600], Loss: 0.1376\n",
      "Epoch [40/50], Step [300/600], Loss: 0.1641\n",
      "Epoch [40/50], Step [400/600], Loss: 0.1269\n",
      "Epoch [40/50], Step [500/600], Loss: 0.1183\n",
      "Epoch [40/50], Step [600/600], Loss: 0.2363\n",
      "Epoch [41/50], Step [100/600], Loss: 0.1060\n",
      "Epoch [41/50], Step [200/600], Loss: 0.1135\n",
      "Epoch [41/50], Step [300/600], Loss: 0.1086\n",
      "Epoch [41/50], Step [400/600], Loss: 0.1576\n",
      "Epoch [41/50], Step [500/600], Loss: 0.1605\n",
      "Epoch [41/50], Step [600/600], Loss: 0.0645\n",
      "Epoch [42/50], Step [100/600], Loss: 0.1493\n",
      "Epoch [42/50], Step [200/600], Loss: 0.1771\n",
      "Epoch [42/50], Step [300/600], Loss: 0.0963\n",
      "Epoch [42/50], Step [400/600], Loss: 0.1375\n",
      "Epoch [42/50], Step [500/600], Loss: 0.1480\n",
      "Epoch [42/50], Step [600/600], Loss: 0.0639\n",
      "Epoch [43/50], Step [100/600], Loss: 0.1110\n",
      "Epoch [43/50], Step [200/600], Loss: 0.1025\n",
      "Epoch [43/50], Step [300/600], Loss: 0.2355\n",
      "Epoch [43/50], Step [400/600], Loss: 0.0709\n",
      "Epoch [43/50], Step [500/600], Loss: 0.0753\n",
      "Epoch [43/50], Step [600/600], Loss: 0.1901\n",
      "Epoch [44/50], Step [100/600], Loss: 0.1759\n",
      "Epoch [44/50], Step [200/600], Loss: 0.1162\n",
      "Epoch [44/50], Step [300/600], Loss: 0.0669\n",
      "Epoch [44/50], Step [400/600], Loss: 0.2212\n",
      "Epoch [44/50], Step [500/600], Loss: 0.1070\n",
      "Epoch [44/50], Step [600/600], Loss: 0.1947\n",
      "Epoch [45/50], Step [100/600], Loss: 0.1487\n",
      "Epoch [45/50], Step [200/600], Loss: 0.1073\n",
      "Epoch [45/50], Step [300/600], Loss: 0.1492\n",
      "Epoch [45/50], Step [400/600], Loss: 0.1205\n",
      "Epoch [45/50], Step [500/600], Loss: 0.1230\n",
      "Epoch [45/50], Step [600/600], Loss: 0.1550\n",
      "Epoch [46/50], Step [100/600], Loss: 0.0982\n",
      "Epoch [46/50], Step [200/600], Loss: 0.0419\n",
      "Epoch [46/50], Step [300/600], Loss: 0.0935\n",
      "Epoch [46/50], Step [400/600], Loss: 0.0516\n",
      "Epoch [46/50], Step [500/600], Loss: 0.0555\n",
      "Epoch [46/50], Step [600/600], Loss: 0.0637\n",
      "Epoch [47/50], Step [100/600], Loss: 0.1228\n",
      "Epoch [47/50], Step [200/600], Loss: 0.1651\n",
      "Epoch [47/50], Step [300/600], Loss: 0.1014\n",
      "Epoch [47/50], Step [400/600], Loss: 0.0866\n",
      "Epoch [47/50], Step [500/600], Loss: 0.0889\n",
      "Epoch [47/50], Step [600/600], Loss: 0.2436\n",
      "Epoch [48/50], Step [100/600], Loss: 0.1231\n",
      "Epoch [48/50], Step [200/600], Loss: 0.2970\n",
      "Epoch [48/50], Step [300/600], Loss: 0.0333\n",
      "Epoch [48/50], Step [400/600], Loss: 0.0850\n",
      "Epoch [48/50], Step [500/600], Loss: 0.1160\n",
      "Epoch [48/50], Step [600/600], Loss: 0.0782\n",
      "Epoch [49/50], Step [100/600], Loss: 0.0896\n",
      "Epoch [49/50], Step [200/600], Loss: 0.1471\n",
      "Epoch [49/50], Step [300/600], Loss: 0.1608\n",
      "Epoch [49/50], Step [400/600], Loss: 0.1793\n",
      "Epoch [49/50], Step [500/600], Loss: 0.1109\n",
      "Epoch [49/50], Step [600/600], Loss: 0.1163\n",
      "Epoch [50/50], Step [100/600], Loss: 0.0513\n",
      "Epoch [50/50], Step [200/600], Loss: 0.1242\n",
      "Epoch [50/50], Step [300/600], Loss: 0.1335\n",
      "Epoch [50/50], Step [400/600], Loss: 0.1575\n",
      "Epoch [50/50], Step [500/600], Loss: 0.1179\n",
      "Epoch [50/50], Step [600/600], Loss: 0.0751\n",
      "Epoch [1/50], Step [100/600], Loss: 1.6770\n",
      "Epoch [1/50], Step [200/600], Loss: 1.0386\n",
      "Epoch [1/50], Step [300/600], Loss: 0.9361\n",
      "Epoch [1/50], Step [400/600], Loss: 0.6298\n",
      "Epoch [1/50], Step [500/600], Loss: 0.6070\n",
      "Epoch [1/50], Step [600/600], Loss: 0.4826\n",
      "Epoch [2/50], Step [100/600], Loss: 0.4677\n",
      "Epoch [2/50], Step [200/600], Loss: 0.5301\n",
      "Epoch [2/50], Step [300/600], Loss: 0.3146\n",
      "Epoch [2/50], Step [400/600], Loss: 0.3698\n",
      "Epoch [2/50], Step [500/600], Loss: 0.2810\n",
      "Epoch [2/50], Step [600/600], Loss: 0.3153\n",
      "Epoch [3/50], Step [100/600], Loss: 0.4780\n",
      "Epoch [3/50], Step [200/600], Loss: 0.3315\n",
      "Epoch [3/50], Step [300/600], Loss: 0.3571\n",
      "Epoch [3/50], Step [400/600], Loss: 0.2926\n",
      "Epoch [3/50], Step [500/600], Loss: 0.3217\n",
      "Epoch [3/50], Step [600/600], Loss: 0.3786\n",
      "Epoch [4/50], Step [100/600], Loss: 0.3050\n",
      "Epoch [4/50], Step [200/600], Loss: 0.2734\n",
      "Epoch [4/50], Step [300/600], Loss: 0.4442\n",
      "Epoch [4/50], Step [400/600], Loss: 0.2672\n",
      "Epoch [4/50], Step [500/600], Loss: 0.3772\n",
      "Epoch [4/50], Step [600/600], Loss: 0.4160\n",
      "Epoch [5/50], Step [100/600], Loss: 0.1951\n",
      "Epoch [5/50], Step [200/600], Loss: 0.2033\n",
      "Epoch [5/50], Step [300/600], Loss: 0.2576\n",
      "Epoch [5/50], Step [400/600], Loss: 0.3608\n",
      "Epoch [5/50], Step [500/600], Loss: 0.2929\n",
      "Epoch [5/50], Step [600/600], Loss: 0.3242\n",
      "Epoch [6/50], Step [100/600], Loss: 0.2053\n",
      "Epoch [6/50], Step [200/600], Loss: 0.2667\n",
      "Epoch [6/50], Step [300/600], Loss: 0.3768\n",
      "Epoch [6/50], Step [400/600], Loss: 0.2675\n",
      "Epoch [6/50], Step [500/600], Loss: 0.2585\n",
      "Epoch [6/50], Step [600/600], Loss: 0.3802\n",
      "Epoch [7/50], Step [100/600], Loss: 0.2648\n",
      "Epoch [7/50], Step [200/600], Loss: 0.2701\n",
      "Epoch [7/50], Step [300/600], Loss: 0.2891\n",
      "Epoch [7/50], Step [400/600], Loss: 0.3722\n",
      "Epoch [7/50], Step [500/600], Loss: 0.2174\n",
      "Epoch [7/50], Step [600/600], Loss: 0.2365\n",
      "Epoch [8/50], Step [100/600], Loss: 0.2163\n",
      "Epoch [8/50], Step [200/600], Loss: 0.2267\n",
      "Epoch [8/50], Step [300/600], Loss: 0.2045\n",
      "Epoch [8/50], Step [400/600], Loss: 0.2587\n",
      "Epoch [8/50], Step [500/600], Loss: 0.1538\n",
      "Epoch [8/50], Step [600/600], Loss: 0.2921\n",
      "Epoch [9/50], Step [100/600], Loss: 0.2208\n",
      "Epoch [9/50], Step [200/600], Loss: 0.2093\n",
      "Epoch [9/50], Step [300/600], Loss: 0.1218\n",
      "Epoch [9/50], Step [400/600], Loss: 0.3327\n",
      "Epoch [9/50], Step [500/600], Loss: 0.1288\n",
      "Epoch [9/50], Step [600/600], Loss: 0.1785\n",
      "Epoch [10/50], Step [100/600], Loss: 0.2948\n",
      "Epoch [10/50], Step [200/600], Loss: 0.2037\n",
      "Epoch [10/50], Step [300/600], Loss: 0.1370\n",
      "Epoch [10/50], Step [400/600], Loss: 0.2238\n",
      "Epoch [10/50], Step [500/600], Loss: 0.1461\n",
      "Epoch [10/50], Step [600/600], Loss: 0.1945\n",
      "Epoch [11/50], Step [100/600], Loss: 0.3176\n",
      "Epoch [11/50], Step [200/600], Loss: 0.3773\n",
      "Epoch [11/50], Step [300/600], Loss: 0.1791\n",
      "Epoch [11/50], Step [400/600], Loss: 0.2314\n",
      "Epoch [11/50], Step [500/600], Loss: 0.2427\n",
      "Epoch [11/50], Step [600/600], Loss: 0.1932\n",
      "Epoch [12/50], Step [100/600], Loss: 0.2081\n",
      "Epoch [12/50], Step [200/600], Loss: 0.1343\n",
      "Epoch [12/50], Step [300/600], Loss: 0.1340\n",
      "Epoch [12/50], Step [400/600], Loss: 0.2372\n",
      "Epoch [12/50], Step [500/600], Loss: 0.1884\n",
      "Epoch [12/50], Step [600/600], Loss: 0.3881\n",
      "Epoch [13/50], Step [100/600], Loss: 0.1352\n",
      "Epoch [13/50], Step [200/600], Loss: 0.1389\n",
      "Epoch [13/50], Step [300/600], Loss: 0.2299\n",
      "Epoch [13/50], Step [400/600], Loss: 0.2090\n",
      "Epoch [13/50], Step [500/600], Loss: 0.1822\n",
      "Epoch [13/50], Step [600/600], Loss: 0.1553\n",
      "Epoch [14/50], Step [100/600], Loss: 0.1178\n",
      "Epoch [14/50], Step [200/600], Loss: 0.1812\n",
      "Epoch [14/50], Step [300/600], Loss: 0.2571\n",
      "Epoch [14/50], Step [400/600], Loss: 0.2058\n",
      "Epoch [14/50], Step [500/600], Loss: 0.1771\n",
      "Epoch [14/50], Step [600/600], Loss: 0.2411\n",
      "Epoch [15/50], Step [100/600], Loss: 0.1152\n",
      "Epoch [15/50], Step [200/600], Loss: 0.1979\n",
      "Epoch [15/50], Step [300/600], Loss: 0.1849\n",
      "Epoch [15/50], Step [400/600], Loss: 0.2416\n",
      "Epoch [15/50], Step [500/600], Loss: 0.1576\n",
      "Epoch [15/50], Step [600/600], Loss: 0.1822\n",
      "Epoch [16/50], Step [100/600], Loss: 0.0984\n",
      "Epoch [16/50], Step [200/600], Loss: 0.1956\n",
      "Epoch [16/50], Step [300/600], Loss: 0.1274\n",
      "Epoch [16/50], Step [400/600], Loss: 0.1127\n",
      "Epoch [16/50], Step [500/600], Loss: 0.2071\n",
      "Epoch [16/50], Step [600/600], Loss: 0.2403\n",
      "Epoch [17/50], Step [100/600], Loss: 0.0981\n",
      "Epoch [17/50], Step [200/600], Loss: 0.1093\n",
      "Epoch [17/50], Step [300/600], Loss: 0.1037\n",
      "Epoch [17/50], Step [400/600], Loss: 0.1701\n",
      "Epoch [17/50], Step [500/600], Loss: 0.2478\n",
      "Epoch [17/50], Step [600/600], Loss: 0.1194\n",
      "Epoch [18/50], Step [100/600], Loss: 0.1323\n",
      "Epoch [18/50], Step [200/600], Loss: 0.1474\n",
      "Epoch [18/50], Step [300/600], Loss: 0.1584\n",
      "Epoch [18/50], Step [400/600], Loss: 0.2430\n",
      "Epoch [18/50], Step [500/600], Loss: 0.1737\n",
      "Epoch [18/50], Step [600/600], Loss: 0.0693\n",
      "Epoch [19/50], Step [100/600], Loss: 0.1643\n",
      "Epoch [19/50], Step [200/600], Loss: 0.2429\n",
      "Epoch [19/50], Step [300/600], Loss: 0.1005\n",
      "Epoch [19/50], Step [400/600], Loss: 0.1009\n",
      "Epoch [19/50], Step [500/600], Loss: 0.1642\n",
      "Epoch [19/50], Step [600/600], Loss: 0.1486\n",
      "Epoch [20/50], Step [100/600], Loss: 0.0548\n",
      "Epoch [20/50], Step [200/600], Loss: 0.1576\n",
      "Epoch [20/50], Step [300/600], Loss: 0.1364\n",
      "Epoch [20/50], Step [400/600], Loss: 0.0912\n",
      "Epoch [20/50], Step [500/600], Loss: 0.1300\n",
      "Epoch [20/50], Step [600/600], Loss: 0.1382\n",
      "Epoch [21/50], Step [100/600], Loss: 0.1090\n",
      "Epoch [21/50], Step [200/600], Loss: 0.1971\n",
      "Epoch [21/50], Step [300/600], Loss: 0.2814\n",
      "Epoch [21/50], Step [400/600], Loss: 0.1453\n",
      "Epoch [21/50], Step [500/600], Loss: 0.1646\n",
      "Epoch [21/50], Step [600/600], Loss: 0.1090\n",
      "Epoch [22/50], Step [100/600], Loss: 0.0589\n",
      "Epoch [22/50], Step [200/600], Loss: 0.1408\n",
      "Epoch [22/50], Step [300/600], Loss: 0.1007\n",
      "Epoch [22/50], Step [400/600], Loss: 0.1601\n",
      "Epoch [22/50], Step [500/600], Loss: 0.0821\n",
      "Epoch [22/50], Step [600/600], Loss: 0.1188\n",
      "Epoch [23/50], Step [100/600], Loss: 0.0835\n",
      "Epoch [23/50], Step [200/600], Loss: 0.1759\n",
      "Epoch [23/50], Step [300/600], Loss: 0.1651\n",
      "Epoch [23/50], Step [400/600], Loss: 0.2013\n",
      "Epoch [23/50], Step [500/600], Loss: 0.1173\n",
      "Epoch [23/50], Step [600/600], Loss: 0.2237\n",
      "Epoch [24/50], Step [100/600], Loss: 0.1217\n",
      "Epoch [24/50], Step [200/600], Loss: 0.1418\n",
      "Epoch [24/50], Step [300/600], Loss: 0.1882\n",
      "Epoch [24/50], Step [400/600], Loss: 0.2157\n",
      "Epoch [24/50], Step [500/600], Loss: 0.1483\n",
      "Epoch [24/50], Step [600/600], Loss: 0.1239\n",
      "Epoch [25/50], Step [100/600], Loss: 0.1781\n",
      "Epoch [25/50], Step [200/600], Loss: 0.1297\n",
      "Epoch [25/50], Step [300/600], Loss: 0.1037\n",
      "Epoch [25/50], Step [400/600], Loss: 0.2243\n",
      "Epoch [25/50], Step [500/600], Loss: 0.1138\n",
      "Epoch [25/50], Step [600/600], Loss: 0.0451\n",
      "Epoch [26/50], Step [100/600], Loss: 0.1388\n",
      "Epoch [26/50], Step [200/600], Loss: 0.0780\n",
      "Epoch [26/50], Step [300/600], Loss: 0.1318\n",
      "Epoch [26/50], Step [400/600], Loss: 0.0760\n",
      "Epoch [26/50], Step [500/600], Loss: 0.1550\n",
      "Epoch [26/50], Step [600/600], Loss: 0.2452\n",
      "Epoch [27/50], Step [100/600], Loss: 0.1372\n",
      "Epoch [27/50], Step [200/600], Loss: 0.1201\n",
      "Epoch [27/50], Step [300/600], Loss: 0.1459\n",
      "Epoch [27/50], Step [400/600], Loss: 0.0603\n",
      "Epoch [27/50], Step [500/600], Loss: 0.1304\n",
      "Epoch [27/50], Step [600/600], Loss: 0.0634\n",
      "Epoch [28/50], Step [100/600], Loss: 0.1358\n",
      "Epoch [28/50], Step [200/600], Loss: 0.1553\n",
      "Epoch [28/50], Step [300/600], Loss: 0.2015\n",
      "Epoch [28/50], Step [400/600], Loss: 0.0902\n",
      "Epoch [28/50], Step [500/600], Loss: 0.1389\n",
      "Epoch [28/50], Step [600/600], Loss: 0.1130\n",
      "Epoch [29/50], Step [100/600], Loss: 0.1001\n",
      "Epoch [29/50], Step [200/600], Loss: 0.1991\n",
      "Epoch [29/50], Step [300/600], Loss: 0.0630\n",
      "Epoch [29/50], Step [400/600], Loss: 0.0870\n",
      "Epoch [29/50], Step [500/600], Loss: 0.0576\n",
      "Epoch [29/50], Step [600/600], Loss: 0.0871\n",
      "Epoch [30/50], Step [100/600], Loss: 0.0679\n",
      "Epoch [30/50], Step [200/600], Loss: 0.1216\n",
      "Epoch [30/50], Step [300/600], Loss: 0.1006\n",
      "Epoch [30/50], Step [400/600], Loss: 0.1947\n",
      "Epoch [30/50], Step [500/600], Loss: 0.1735\n",
      "Epoch [30/50], Step [600/600], Loss: 0.0803\n",
      "Epoch [31/50], Step [100/600], Loss: 0.1019\n",
      "Epoch [31/50], Step [200/600], Loss: 0.0843\n",
      "Epoch [31/50], Step [300/600], Loss: 0.0584\n",
      "Epoch [31/50], Step [400/600], Loss: 0.1405\n",
      "Epoch [31/50], Step [500/600], Loss: 0.1201\n",
      "Epoch [31/50], Step [600/600], Loss: 0.1207\n",
      "Epoch [32/50], Step [100/600], Loss: 0.0747\n",
      "Epoch [32/50], Step [200/600], Loss: 0.1367\n",
      "Epoch [32/50], Step [300/600], Loss: 0.0868\n",
      "Epoch [32/50], Step [400/600], Loss: 0.1702\n",
      "Epoch [32/50], Step [500/600], Loss: 0.1241\n",
      "Epoch [32/50], Step [600/600], Loss: 0.0773\n",
      "Epoch [33/50], Step [100/600], Loss: 0.1932\n",
      "Epoch [33/50], Step [200/600], Loss: 0.1503\n",
      "Epoch [33/50], Step [300/600], Loss: 0.0531\n",
      "Epoch [33/50], Step [400/600], Loss: 0.0862\n",
      "Epoch [33/50], Step [500/600], Loss: 0.0788\n",
      "Epoch [33/50], Step [600/600], Loss: 0.1051\n",
      "Epoch [34/50], Step [100/600], Loss: 0.0934\n",
      "Epoch [34/50], Step [200/600], Loss: 0.0876\n",
      "Epoch [34/50], Step [300/600], Loss: 0.0452\n",
      "Epoch [34/50], Step [400/600], Loss: 0.1148\n",
      "Epoch [34/50], Step [500/600], Loss: 0.1717\n",
      "Epoch [34/50], Step [600/600], Loss: 0.0853\n",
      "Epoch [35/50], Step [100/600], Loss: 0.0869\n",
      "Epoch [35/50], Step [200/600], Loss: 0.0961\n",
      "Epoch [35/50], Step [300/600], Loss: 0.1224\n",
      "Epoch [35/50], Step [400/600], Loss: 0.0564\n",
      "Epoch [35/50], Step [500/600], Loss: 0.0679\n",
      "Epoch [35/50], Step [600/600], Loss: 0.3097\n",
      "Epoch [36/50], Step [100/600], Loss: 0.0716\n",
      "Epoch [36/50], Step [200/600], Loss: 0.0767\n",
      "Epoch [36/50], Step [300/600], Loss: 0.1178\n",
      "Epoch [36/50], Step [400/600], Loss: 0.1692\n",
      "Epoch [36/50], Step [500/600], Loss: 0.0458\n",
      "Epoch [36/50], Step [600/600], Loss: 0.1292\n",
      "Epoch [37/50], Step [100/600], Loss: 0.0993\n",
      "Epoch [37/50], Step [200/600], Loss: 0.0970\n",
      "Epoch [37/50], Step [300/600], Loss: 0.1675\n",
      "Epoch [37/50], Step [400/600], Loss: 0.0934\n",
      "Epoch [37/50], Step [500/600], Loss: 0.0891\n",
      "Epoch [37/50], Step [600/600], Loss: 0.0599\n",
      "Epoch [38/50], Step [100/600], Loss: 0.1541\n",
      "Epoch [38/50], Step [200/600], Loss: 0.0722\n",
      "Epoch [38/50], Step [300/600], Loss: 0.0678\n",
      "Epoch [38/50], Step [400/600], Loss: 0.1251\n",
      "Epoch [38/50], Step [500/600], Loss: 0.0982\n",
      "Epoch [38/50], Step [600/600], Loss: 0.1440\n",
      "Epoch [39/50], Step [100/600], Loss: 0.1285\n",
      "Epoch [39/50], Step [200/600], Loss: 0.1336\n",
      "Epoch [39/50], Step [300/600], Loss: 0.1901\n",
      "Epoch [39/50], Step [400/600], Loss: 0.1253\n",
      "Epoch [39/50], Step [500/600], Loss: 0.0681\n",
      "Epoch [39/50], Step [600/600], Loss: 0.1064\n",
      "Epoch [40/50], Step [100/600], Loss: 0.1274\n",
      "Epoch [40/50], Step [200/600], Loss: 0.0470\n",
      "Epoch [40/50], Step [300/600], Loss: 0.0742\n",
      "Epoch [40/50], Step [400/600], Loss: 0.1032\n",
      "Epoch [40/50], Step [500/600], Loss: 0.0785\n",
      "Epoch [40/50], Step [600/600], Loss: 0.1094\n",
      "Epoch [41/50], Step [100/600], Loss: 0.1447\n",
      "Epoch [41/50], Step [200/600], Loss: 0.0940\n",
      "Epoch [41/50], Step [300/600], Loss: 0.1174\n",
      "Epoch [41/50], Step [400/600], Loss: 0.1169\n",
      "Epoch [41/50], Step [500/600], Loss: 0.0852\n",
      "Epoch [41/50], Step [600/600], Loss: 0.0418\n",
      "Epoch [42/50], Step [100/600], Loss: 0.0874\n",
      "Epoch [42/50], Step [200/600], Loss: 0.0954\n",
      "Epoch [42/50], Step [300/600], Loss: 0.1203\n",
      "Epoch [42/50], Step [400/600], Loss: 0.0884\n",
      "Epoch [42/50], Step [500/600], Loss: 0.0503\n",
      "Epoch [42/50], Step [600/600], Loss: 0.1300\n",
      "Epoch [43/50], Step [100/600], Loss: 0.0473\n",
      "Epoch [43/50], Step [200/600], Loss: 0.0588\n",
      "Epoch [43/50], Step [300/600], Loss: 0.0523\n",
      "Epoch [43/50], Step [400/600], Loss: 0.1688\n",
      "Epoch [43/50], Step [500/600], Loss: 0.0488\n",
      "Epoch [43/50], Step [600/600], Loss: 0.1201\n",
      "Epoch [44/50], Step [100/600], Loss: 0.1170\n",
      "Epoch [44/50], Step [200/600], Loss: 0.0863\n",
      "Epoch [44/50], Step [300/600], Loss: 0.1721\n",
      "Epoch [44/50], Step [400/600], Loss: 0.1538\n",
      "Epoch [44/50], Step [500/600], Loss: 0.0782\n",
      "Epoch [44/50], Step [600/600], Loss: 0.0558\n",
      "Epoch [45/50], Step [100/600], Loss: 0.1088\n",
      "Epoch [45/50], Step [200/600], Loss: 0.0649\n",
      "Epoch [45/50], Step [300/600], Loss: 0.1051\n",
      "Epoch [45/50], Step [400/600], Loss: 0.1191\n",
      "Epoch [45/50], Step [500/600], Loss: 0.0918\n",
      "Epoch [45/50], Step [600/600], Loss: 0.2532\n",
      "Epoch [46/50], Step [100/600], Loss: 0.0950\n",
      "Epoch [46/50], Step [200/600], Loss: 0.0289\n",
      "Epoch [46/50], Step [300/600], Loss: 0.0952\n",
      "Epoch [46/50], Step [400/600], Loss: 0.1084\n",
      "Epoch [46/50], Step [500/600], Loss: 0.0857\n",
      "Epoch [46/50], Step [600/600], Loss: 0.0689\n",
      "Epoch [47/50], Step [100/600], Loss: 0.0807\n",
      "Epoch [47/50], Step [200/600], Loss: 0.0465\n",
      "Epoch [47/50], Step [300/600], Loss: 0.1283\n",
      "Epoch [47/50], Step [400/600], Loss: 0.1120\n",
      "Epoch [47/50], Step [500/600], Loss: 0.1730\n",
      "Epoch [47/50], Step [600/600], Loss: 0.0863\n",
      "Epoch [48/50], Step [100/600], Loss: 0.0428\n",
      "Epoch [48/50], Step [200/600], Loss: 0.1535\n",
      "Epoch [48/50], Step [300/600], Loss: 0.1390\n",
      "Epoch [48/50], Step [400/600], Loss: 0.1007\n",
      "Epoch [48/50], Step [500/600], Loss: 0.1366\n",
      "Epoch [48/50], Step [600/600], Loss: 0.0377\n",
      "Epoch [49/50], Step [100/600], Loss: 0.0354\n",
      "Epoch [49/50], Step [200/600], Loss: 0.1589\n",
      "Epoch [49/50], Step [300/600], Loss: 0.0843\n",
      "Epoch [49/50], Step [400/600], Loss: 0.0576\n",
      "Epoch [49/50], Step [500/600], Loss: 0.1394\n",
      "Epoch [49/50], Step [600/600], Loss: 0.0895\n",
      "Epoch [50/50], Step [100/600], Loss: 0.0849\n",
      "Epoch [50/50], Step [200/600], Loss: 0.1233\n",
      "Epoch [50/50], Step [300/600], Loss: 0.0492\n",
      "Epoch [50/50], Step [400/600], Loss: 0.0885\n",
      "Epoch [50/50], Step [500/600], Loss: 0.0431\n",
      "Epoch [50/50], Step [600/600], Loss: 0.0923\n",
      "Epoch [1/50], Step [100/600], Loss: 1.6926\n",
      "Epoch [1/50], Step [200/600], Loss: 1.0676\n",
      "Epoch [1/50], Step [300/600], Loss: 0.8940\n",
      "Epoch [1/50], Step [400/600], Loss: 0.6447\n",
      "Epoch [1/50], Step [500/600], Loss: 0.5944\n",
      "Epoch [1/50], Step [600/600], Loss: 0.6229\n",
      "Epoch [2/50], Step [100/600], Loss: 0.4415\n",
      "Epoch [2/50], Step [200/600], Loss: 0.5221\n",
      "Epoch [2/50], Step [300/600], Loss: 0.4783\n",
      "Epoch [2/50], Step [400/600], Loss: 0.5635\n",
      "Epoch [2/50], Step [500/600], Loss: 0.3486\n",
      "Epoch [2/50], Step [600/600], Loss: 0.3159\n",
      "Epoch [3/50], Step [100/600], Loss: 0.2316\n",
      "Epoch [3/50], Step [200/600], Loss: 0.5740\n",
      "Epoch [3/50], Step [300/600], Loss: 0.3704\n",
      "Epoch [3/50], Step [400/600], Loss: 0.3923\n",
      "Epoch [3/50], Step [500/600], Loss: 0.2033\n",
      "Epoch [3/50], Step [600/600], Loss: 0.2415\n",
      "Epoch [4/50], Step [100/600], Loss: 0.4105\n",
      "Epoch [4/50], Step [200/600], Loss: 0.3824\n",
      "Epoch [4/50], Step [300/600], Loss: 0.2445\n",
      "Epoch [4/50], Step [400/600], Loss: 0.4475\n",
      "Epoch [4/50], Step [500/600], Loss: 0.4152\n",
      "Epoch [4/50], Step [600/600], Loss: 0.2734\n",
      "Epoch [5/50], Step [100/600], Loss: 0.2478\n",
      "Epoch [5/50], Step [200/600], Loss: 0.3236\n",
      "Epoch [5/50], Step [300/600], Loss: 0.3043\n",
      "Epoch [5/50], Step [400/600], Loss: 0.2912\n",
      "Epoch [5/50], Step [500/600], Loss: 0.4094\n",
      "Epoch [5/50], Step [600/600], Loss: 0.2032\n",
      "Epoch [6/50], Step [100/600], Loss: 0.2166\n",
      "Epoch [6/50], Step [200/600], Loss: 0.3605\n",
      "Epoch [6/50], Step [300/600], Loss: 0.3027\n",
      "Epoch [6/50], Step [400/600], Loss: 0.3631\n",
      "Epoch [6/50], Step [500/600], Loss: 0.2841\n",
      "Epoch [6/50], Step [600/600], Loss: 0.2575\n",
      "Epoch [7/50], Step [100/600], Loss: 0.2752\n",
      "Epoch [7/50], Step [200/600], Loss: 0.1299\n",
      "Epoch [7/50], Step [300/600], Loss: 0.2480\n",
      "Epoch [7/50], Step [400/600], Loss: 0.1672\n",
      "Epoch [7/50], Step [500/600], Loss: 0.3464\n",
      "Epoch [7/50], Step [600/600], Loss: 0.3697\n",
      "Epoch [8/50], Step [100/600], Loss: 0.2995\n",
      "Epoch [8/50], Step [200/600], Loss: 0.3363\n",
      "Epoch [8/50], Step [300/600], Loss: 0.2523\n",
      "Epoch [8/50], Step [400/600], Loss: 0.4347\n",
      "Epoch [8/50], Step [500/600], Loss: 0.1467\n",
      "Epoch [8/50], Step [600/600], Loss: 0.2004\n",
      "Epoch [9/50], Step [100/600], Loss: 0.1974\n",
      "Epoch [9/50], Step [200/600], Loss: 0.2206\n",
      "Epoch [9/50], Step [300/600], Loss: 0.3055\n",
      "Epoch [9/50], Step [400/600], Loss: 0.1812\n",
      "Epoch [9/50], Step [500/600], Loss: 0.2208\n",
      "Epoch [9/50], Step [600/600], Loss: 0.1771\n",
      "Epoch [10/50], Step [100/600], Loss: 0.2157\n",
      "Epoch [10/50], Step [200/600], Loss: 0.1618\n",
      "Epoch [10/50], Step [300/600], Loss: 0.1491\n",
      "Epoch [10/50], Step [400/600], Loss: 0.3551\n",
      "Epoch [10/50], Step [500/600], Loss: 0.2280\n",
      "Epoch [10/50], Step [600/600], Loss: 0.2246\n",
      "Epoch [11/50], Step [100/600], Loss: 0.1446\n",
      "Epoch [11/50], Step [200/600], Loss: 0.1017\n",
      "Epoch [11/50], Step [300/600], Loss: 0.2360\n",
      "Epoch [11/50], Step [400/600], Loss: 0.1566\n",
      "Epoch [11/50], Step [500/600], Loss: 0.2022\n",
      "Epoch [11/50], Step [600/600], Loss: 0.1896\n",
      "Epoch [12/50], Step [100/600], Loss: 0.1282\n",
      "Epoch [12/50], Step [200/600], Loss: 0.1461\n",
      "Epoch [12/50], Step [300/600], Loss: 0.1536\n",
      "Epoch [12/50], Step [400/600], Loss: 0.1646\n",
      "Epoch [12/50], Step [500/600], Loss: 0.1626\n",
      "Epoch [12/50], Step [600/600], Loss: 0.1400\n",
      "Epoch [13/50], Step [100/600], Loss: 0.1672\n",
      "Epoch [13/50], Step [200/600], Loss: 0.2005\n",
      "Epoch [13/50], Step [300/600], Loss: 0.1023\n",
      "Epoch [13/50], Step [400/600], Loss: 0.1411\n",
      "Epoch [13/50], Step [500/600], Loss: 0.2633\n",
      "Epoch [13/50], Step [600/600], Loss: 0.2336\n",
      "Epoch [14/50], Step [100/600], Loss: 0.2627\n",
      "Epoch [14/50], Step [200/600], Loss: 0.1384\n",
      "Epoch [14/50], Step [300/600], Loss: 0.1187\n",
      "Epoch [14/50], Step [400/600], Loss: 0.1564\n",
      "Epoch [14/50], Step [500/600], Loss: 0.1912\n",
      "Epoch [14/50], Step [600/600], Loss: 0.1704\n",
      "Epoch [15/50], Step [100/600], Loss: 0.1634\n",
      "Epoch [15/50], Step [200/600], Loss: 0.2013\n",
      "Epoch [15/50], Step [300/600], Loss: 0.2705\n",
      "Epoch [15/50], Step [400/600], Loss: 0.2007\n",
      "Epoch [15/50], Step [500/600], Loss: 0.1392\n",
      "Epoch [15/50], Step [600/600], Loss: 0.3459\n",
      "Epoch [16/50], Step [100/600], Loss: 0.1865\n",
      "Epoch [16/50], Step [200/600], Loss: 0.1662\n",
      "Epoch [16/50], Step [300/600], Loss: 0.2085\n",
      "Epoch [16/50], Step [400/600], Loss: 0.2095\n",
      "Epoch [16/50], Step [500/600], Loss: 0.0895\n",
      "Epoch [16/50], Step [600/600], Loss: 0.1394\n",
      "Epoch [17/50], Step [100/600], Loss: 0.3499\n",
      "Epoch [17/50], Step [200/600], Loss: 0.1968\n",
      "Epoch [17/50], Step [300/600], Loss: 0.1148\n",
      "Epoch [17/50], Step [400/600], Loss: 0.2752\n",
      "Epoch [17/50], Step [500/600], Loss: 0.1850\n",
      "Epoch [17/50], Step [600/600], Loss: 0.2023\n",
      "Epoch [18/50], Step [100/600], Loss: 0.1565\n",
      "Epoch [18/50], Step [200/600], Loss: 0.1914\n",
      "Epoch [18/50], Step [300/600], Loss: 0.3196\n",
      "Epoch [18/50], Step [400/600], Loss: 0.1836\n",
      "Epoch [18/50], Step [500/600], Loss: 0.0756\n",
      "Epoch [18/50], Step [600/600], Loss: 0.0981\n",
      "Epoch [19/50], Step [100/600], Loss: 0.2020\n",
      "Epoch [19/50], Step [200/600], Loss: 0.0985\n",
      "Epoch [19/50], Step [300/600], Loss: 0.1414\n",
      "Epoch [19/50], Step [400/600], Loss: 0.1383\n",
      "Epoch [19/50], Step [500/600], Loss: 0.1085\n",
      "Epoch [19/50], Step [600/600], Loss: 0.1189\n",
      "Epoch [20/50], Step [100/600], Loss: 0.1275\n",
      "Epoch [20/50], Step [200/600], Loss: 0.1088\n",
      "Epoch [20/50], Step [300/600], Loss: 0.1203\n",
      "Epoch [20/50], Step [400/600], Loss: 0.1343\n",
      "Epoch [20/50], Step [500/600], Loss: 0.1484\n",
      "Epoch [20/50], Step [600/600], Loss: 0.1101\n",
      "Epoch [21/50], Step [100/600], Loss: 0.2104\n",
      "Epoch [21/50], Step [200/600], Loss: 0.1020\n",
      "Epoch [21/50], Step [300/600], Loss: 0.1580\n",
      "Epoch [21/50], Step [400/600], Loss: 0.1659\n",
      "Epoch [21/50], Step [500/600], Loss: 0.0941\n",
      "Epoch [21/50], Step [600/600], Loss: 0.1365\n",
      "Epoch [22/50], Step [100/600], Loss: 0.1497\n",
      "Epoch [22/50], Step [200/600], Loss: 0.2569\n",
      "Epoch [22/50], Step [300/600], Loss: 0.1545\n",
      "Epoch [22/50], Step [400/600], Loss: 0.0917\n",
      "Epoch [22/50], Step [500/600], Loss: 0.0830\n",
      "Epoch [22/50], Step [600/600], Loss: 0.1823\n",
      "Epoch [23/50], Step [100/600], Loss: 0.0360\n",
      "Epoch [23/50], Step [200/600], Loss: 0.0955\n",
      "Epoch [23/50], Step [300/600], Loss: 0.2799\n",
      "Epoch [23/50], Step [400/600], Loss: 0.1753\n",
      "Epoch [23/50], Step [500/600], Loss: 0.1385\n",
      "Epoch [23/50], Step [600/600], Loss: 0.2125\n",
      "Epoch [24/50], Step [100/600], Loss: 0.1094\n",
      "Epoch [24/50], Step [200/600], Loss: 0.1220\n",
      "Epoch [24/50], Step [300/600], Loss: 0.1797\n",
      "Epoch [24/50], Step [400/600], Loss: 0.1050\n",
      "Epoch [24/50], Step [500/600], Loss: 0.1036\n",
      "Epoch [24/50], Step [600/600], Loss: 0.0912\n",
      "Epoch [25/50], Step [100/600], Loss: 0.0897\n",
      "Epoch [25/50], Step [200/600], Loss: 0.1420\n",
      "Epoch [25/50], Step [300/600], Loss: 0.1150\n",
      "Epoch [25/50], Step [400/600], Loss: 0.0793\n",
      "Epoch [25/50], Step [500/600], Loss: 0.0944\n",
      "Epoch [25/50], Step [600/600], Loss: 0.1083\n",
      "Epoch [26/50], Step [100/600], Loss: 0.1554\n",
      "Epoch [26/50], Step [200/600], Loss: 0.0902\n",
      "Epoch [26/50], Step [300/600], Loss: 0.1781\n",
      "Epoch [26/50], Step [400/600], Loss: 0.0905\n",
      "Epoch [26/50], Step [500/600], Loss: 0.1217\n",
      "Epoch [26/50], Step [600/600], Loss: 0.2215\n",
      "Epoch [27/50], Step [100/600], Loss: 0.1305\n",
      "Epoch [27/50], Step [200/600], Loss: 0.0838\n",
      "Epoch [27/50], Step [300/600], Loss: 0.1678\n",
      "Epoch [27/50], Step [400/600], Loss: 0.1408\n",
      "Epoch [27/50], Step [500/600], Loss: 0.1564\n",
      "Epoch [27/50], Step [600/600], Loss: 0.0888\n",
      "Epoch [28/50], Step [100/600], Loss: 0.1792\n",
      "Epoch [28/50], Step [200/600], Loss: 0.2050\n",
      "Epoch [28/50], Step [300/600], Loss: 0.1188\n",
      "Epoch [28/50], Step [400/600], Loss: 0.0779\n",
      "Epoch [28/50], Step [500/600], Loss: 0.0525\n",
      "Epoch [28/50], Step [600/600], Loss: 0.1190\n",
      "Epoch [29/50], Step [100/600], Loss: 0.1221\n",
      "Epoch [29/50], Step [200/600], Loss: 0.1106\n",
      "Epoch [29/50], Step [300/600], Loss: 0.0602\n",
      "Epoch [29/50], Step [400/600], Loss: 0.0610\n",
      "Epoch [29/50], Step [500/600], Loss: 0.0558\n",
      "Epoch [29/50], Step [600/600], Loss: 0.1069\n",
      "Epoch [30/50], Step [100/600], Loss: 0.1815\n",
      "Epoch [30/50], Step [200/600], Loss: 0.2552\n",
      "Epoch [30/50], Step [300/600], Loss: 0.1407\n",
      "Epoch [30/50], Step [400/600], Loss: 0.1429\n",
      "Epoch [30/50], Step [500/600], Loss: 0.0816\n",
      "Epoch [30/50], Step [600/600], Loss: 0.1902\n",
      "Epoch [31/50], Step [100/600], Loss: 0.1542\n",
      "Epoch [31/50], Step [200/600], Loss: 0.0245\n",
      "Epoch [31/50], Step [300/600], Loss: 0.1083\n",
      "Epoch [31/50], Step [400/600], Loss: 0.2095\n",
      "Epoch [31/50], Step [500/600], Loss: 0.0877\n",
      "Epoch [31/50], Step [600/600], Loss: 0.1445\n",
      "Epoch [32/50], Step [100/600], Loss: 0.1419\n",
      "Epoch [32/50], Step [200/600], Loss: 0.0987\n",
      "Epoch [32/50], Step [300/600], Loss: 0.1296\n",
      "Epoch [32/50], Step [400/600], Loss: 0.1410\n",
      "Epoch [32/50], Step [500/600], Loss: 0.0790\n",
      "Epoch [32/50], Step [600/600], Loss: 0.1807\n",
      "Epoch [33/50], Step [100/600], Loss: 0.1142\n",
      "Epoch [33/50], Step [200/600], Loss: 0.0708\n",
      "Epoch [33/50], Step [300/600], Loss: 0.0679\n",
      "Epoch [33/50], Step [400/600], Loss: 0.1457\n",
      "Epoch [33/50], Step [500/600], Loss: 0.1484\n",
      "Epoch [33/50], Step [600/600], Loss: 0.1247\n",
      "Epoch [34/50], Step [100/600], Loss: 0.1066\n",
      "Epoch [34/50], Step [200/600], Loss: 0.1720\n",
      "Epoch [34/50], Step [300/600], Loss: 0.0937\n",
      "Epoch [34/50], Step [400/600], Loss: 0.1129\n",
      "Epoch [34/50], Step [500/600], Loss: 0.0932\n",
      "Epoch [34/50], Step [600/600], Loss: 0.0825\n",
      "Epoch [35/50], Step [100/600], Loss: 0.0847\n",
      "Epoch [35/50], Step [200/600], Loss: 0.0992\n",
      "Epoch [35/50], Step [300/600], Loss: 0.1411\n",
      "Epoch [35/50], Step [400/600], Loss: 0.0862\n",
      "Epoch [35/50], Step [500/600], Loss: 0.1111\n",
      "Epoch [35/50], Step [600/600], Loss: 0.0950\n",
      "Epoch [36/50], Step [100/600], Loss: 0.0678\n",
      "Epoch [36/50], Step [200/600], Loss: 0.2140\n",
      "Epoch [36/50], Step [300/600], Loss: 0.1346\n",
      "Epoch [36/50], Step [400/600], Loss: 0.1058\n",
      "Epoch [36/50], Step [500/600], Loss: 0.0441\n",
      "Epoch [36/50], Step [600/600], Loss: 0.0857\n",
      "Epoch [37/50], Step [100/600], Loss: 0.0617\n",
      "Epoch [37/50], Step [200/600], Loss: 0.1378\n",
      "Epoch [37/50], Step [300/600], Loss: 0.0387\n",
      "Epoch [37/50], Step [400/600], Loss: 0.1361\n",
      "Epoch [37/50], Step [500/600], Loss: 0.0348\n",
      "Epoch [37/50], Step [600/600], Loss: 0.1812\n",
      "Epoch [38/50], Step [100/600], Loss: 0.0249\n",
      "Epoch [38/50], Step [200/600], Loss: 0.0721\n",
      "Epoch [38/50], Step [300/600], Loss: 0.0583\n",
      "Epoch [38/50], Step [400/600], Loss: 0.1550\n",
      "Epoch [38/50], Step [500/600], Loss: 0.0598\n",
      "Epoch [38/50], Step [600/600], Loss: 0.0763\n",
      "Epoch [39/50], Step [100/600], Loss: 0.1610\n",
      "Epoch [39/50], Step [200/600], Loss: 0.2165\n",
      "Epoch [39/50], Step [300/600], Loss: 0.1074\n",
      "Epoch [39/50], Step [400/600], Loss: 0.1376\n",
      "Epoch [39/50], Step [500/600], Loss: 0.0637\n",
      "Epoch [39/50], Step [600/600], Loss: 0.1874\n",
      "Epoch [40/50], Step [100/600], Loss: 0.0749\n",
      "Epoch [40/50], Step [200/600], Loss: 0.0776\n",
      "Epoch [40/50], Step [300/600], Loss: 0.0627\n",
      "Epoch [40/50], Step [400/600], Loss: 0.0931\n",
      "Epoch [40/50], Step [500/600], Loss: 0.0594\n",
      "Epoch [40/50], Step [600/600], Loss: 0.1598\n",
      "Epoch [41/50], Step [100/600], Loss: 0.0414\n",
      "Epoch [41/50], Step [200/600], Loss: 0.0962\n",
      "Epoch [41/50], Step [300/600], Loss: 0.1406\n",
      "Epoch [41/50], Step [400/600], Loss: 0.1023\n",
      "Epoch [41/50], Step [500/600], Loss: 0.0866\n",
      "Epoch [41/50], Step [600/600], Loss: 0.0463\n",
      "Epoch [42/50], Step [100/600], Loss: 0.0495\n",
      "Epoch [42/50], Step [200/600], Loss: 0.0855\n",
      "Epoch [42/50], Step [300/600], Loss: 0.0708\n",
      "Epoch [42/50], Step [400/600], Loss: 0.0890\n",
      "Epoch [42/50], Step [500/600], Loss: 0.1250\n",
      "Epoch [42/50], Step [600/600], Loss: 0.0721\n",
      "Epoch [43/50], Step [100/600], Loss: 0.0761\n",
      "Epoch [43/50], Step [200/600], Loss: 0.1278\n",
      "Epoch [43/50], Step [300/600], Loss: 0.0906\n",
      "Epoch [43/50], Step [400/600], Loss: 0.0647\n",
      "Epoch [43/50], Step [500/600], Loss: 0.1149\n",
      "Epoch [43/50], Step [600/600], Loss: 0.0563\n",
      "Epoch [44/50], Step [100/600], Loss: 0.1298\n",
      "Epoch [44/50], Step [200/600], Loss: 0.0607\n",
      "Epoch [44/50], Step [300/600], Loss: 0.0712\n",
      "Epoch [44/50], Step [400/600], Loss: 0.1339\n",
      "Epoch [44/50], Step [500/600], Loss: 0.1676\n",
      "Epoch [44/50], Step [600/600], Loss: 0.1005\n",
      "Epoch [45/50], Step [100/600], Loss: 0.1771\n",
      "Epoch [45/50], Step [200/600], Loss: 0.0308\n",
      "Epoch [45/50], Step [300/600], Loss: 0.1215\n",
      "Epoch [45/50], Step [400/600], Loss: 0.0330\n",
      "Epoch [45/50], Step [500/600], Loss: 0.0667\n",
      "Epoch [45/50], Step [600/600], Loss: 0.0825\n",
      "Epoch [46/50], Step [100/600], Loss: 0.1144\n",
      "Epoch [46/50], Step [200/600], Loss: 0.1425\n",
      "Epoch [46/50], Step [300/600], Loss: 0.1189\n",
      "Epoch [46/50], Step [400/600], Loss: 0.1104\n",
      "Epoch [46/50], Step [500/600], Loss: 0.1220\n",
      "Epoch [46/50], Step [600/600], Loss: 0.0910\n",
      "Epoch [47/50], Step [100/600], Loss: 0.1511\n",
      "Epoch [47/50], Step [200/600], Loss: 0.1295\n",
      "Epoch [47/50], Step [300/600], Loss: 0.1849\n",
      "Epoch [47/50], Step [400/600], Loss: 0.0935\n",
      "Epoch [47/50], Step [500/600], Loss: 0.0744\n",
      "Epoch [47/50], Step [600/600], Loss: 0.0493\n",
      "Epoch [48/50], Step [100/600], Loss: 0.1716\n",
      "Epoch [48/50], Step [200/600], Loss: 0.1253\n",
      "Epoch [48/50], Step [300/600], Loss: 0.0603\n",
      "Epoch [48/50], Step [400/600], Loss: 0.0856\n",
      "Epoch [48/50], Step [500/600], Loss: 0.0597\n",
      "Epoch [48/50], Step [600/600], Loss: 0.0568\n",
      "Epoch [49/50], Step [100/600], Loss: 0.1595\n",
      "Epoch [49/50], Step [200/600], Loss: 0.0501\n",
      "Epoch [49/50], Step [300/600], Loss: 0.1191\n",
      "Epoch [49/50], Step [400/600], Loss: 0.0743\n",
      "Epoch [49/50], Step [500/600], Loss: 0.0965\n",
      "Epoch [49/50], Step [600/600], Loss: 0.0662\n",
      "Epoch [50/50], Step [100/600], Loss: 0.1200\n",
      "Epoch [50/50], Step [200/600], Loss: 0.0850\n",
      "Epoch [50/50], Step [300/600], Loss: 0.1396\n",
      "Epoch [50/50], Step [400/600], Loss: 0.0786\n",
      "Epoch [50/50], Step [500/600], Loss: 0.0469\n",
      "Epoch [50/50], Step [600/600], Loss: 0.0918\n",
      "Epoch [1/50], Step [100/600], Loss: 1.6260\n",
      "Epoch [1/50], Step [200/600], Loss: 1.0172\n",
      "Epoch [1/50], Step [300/600], Loss: 0.8154\n",
      "Epoch [1/50], Step [400/600], Loss: 0.6601\n",
      "Epoch [1/50], Step [500/600], Loss: 0.5605\n",
      "Epoch [1/50], Step [600/600], Loss: 0.4915\n",
      "Epoch [2/50], Step [100/600], Loss: 0.3903\n",
      "Epoch [2/50], Step [200/600], Loss: 0.4868\n",
      "Epoch [2/50], Step [300/600], Loss: 0.4187\n",
      "Epoch [2/50], Step [400/600], Loss: 0.3422\n",
      "Epoch [2/50], Step [500/600], Loss: 0.3999\n",
      "Epoch [2/50], Step [600/600], Loss: 0.2792\n",
      "Epoch [3/50], Step [100/600], Loss: 0.3823\n",
      "Epoch [3/50], Step [200/600], Loss: 0.4678\n",
      "Epoch [3/50], Step [300/600], Loss: 0.3875\n",
      "Epoch [3/50], Step [400/600], Loss: 0.2701\n",
      "Epoch [3/50], Step [500/600], Loss: 0.2829\n",
      "Epoch [3/50], Step [600/600], Loss: 0.3062\n",
      "Epoch [4/50], Step [100/600], Loss: 0.4927\n",
      "Epoch [4/50], Step [200/600], Loss: 0.2649\n",
      "Epoch [4/50], Step [300/600], Loss: 0.3589\n",
      "Epoch [4/50], Step [400/600], Loss: 0.4352\n",
      "Epoch [4/50], Step [500/600], Loss: 0.3915\n",
      "Epoch [4/50], Step [600/600], Loss: 0.2432\n",
      "Epoch [5/50], Step [100/600], Loss: 0.3796\n",
      "Epoch [5/50], Step [200/600], Loss: 0.1608\n",
      "Epoch [5/50], Step [300/600], Loss: 0.3424\n",
      "Epoch [5/50], Step [400/600], Loss: 0.2650\n",
      "Epoch [5/50], Step [500/600], Loss: 0.3020\n",
      "Epoch [5/50], Step [600/600], Loss: 0.1664\n",
      "Epoch [6/50], Step [100/600], Loss: 0.3336\n",
      "Epoch [6/50], Step [200/600], Loss: 0.2887\n",
      "Epoch [6/50], Step [300/600], Loss: 0.2428\n",
      "Epoch [6/50], Step [400/600], Loss: 0.2153\n",
      "Epoch [6/50], Step [500/600], Loss: 0.1807\n",
      "Epoch [6/50], Step [600/600], Loss: 0.3965\n",
      "Epoch [7/50], Step [100/600], Loss: 0.3916\n",
      "Epoch [7/50], Step [200/600], Loss: 0.2466\n",
      "Epoch [7/50], Step [300/600], Loss: 0.2174\n",
      "Epoch [7/50], Step [400/600], Loss: 0.2294\n",
      "Epoch [7/50], Step [500/600], Loss: 0.2127\n",
      "Epoch [7/50], Step [600/600], Loss: 0.4324\n",
      "Epoch [8/50], Step [100/600], Loss: 0.2362\n",
      "Epoch [8/50], Step [200/600], Loss: 0.3516\n",
      "Epoch [8/50], Step [300/600], Loss: 0.1579\n",
      "Epoch [8/50], Step [400/600], Loss: 0.2263\n",
      "Epoch [8/50], Step [500/600], Loss: 0.2411\n",
      "Epoch [8/50], Step [600/600], Loss: 0.1884\n",
      "Epoch [9/50], Step [100/600], Loss: 0.2781\n",
      "Epoch [9/50], Step [200/600], Loss: 0.2313\n",
      "Epoch [9/50], Step [300/600], Loss: 0.1903\n",
      "Epoch [9/50], Step [400/600], Loss: 0.4002\n",
      "Epoch [9/50], Step [500/600], Loss: 0.1523\n",
      "Epoch [9/50], Step [600/600], Loss: 0.3996\n",
      "Epoch [10/50], Step [100/600], Loss: 0.1344\n",
      "Epoch [10/50], Step [200/600], Loss: 0.1388\n",
      "Epoch [10/50], Step [300/600], Loss: 0.3095\n",
      "Epoch [10/50], Step [400/600], Loss: 0.2814\n",
      "Epoch [10/50], Step [500/600], Loss: 0.1405\n",
      "Epoch [10/50], Step [600/600], Loss: 0.1462\n",
      "Epoch [11/50], Step [100/600], Loss: 0.1867\n",
      "Epoch [11/50], Step [200/600], Loss: 0.1333\n",
      "Epoch [11/50], Step [300/600], Loss: 0.1663\n",
      "Epoch [11/50], Step [400/600], Loss: 0.1664\n",
      "Epoch [11/50], Step [500/600], Loss: 0.1640\n",
      "Epoch [11/50], Step [600/600], Loss: 0.2073\n",
      "Epoch [12/50], Step [100/600], Loss: 0.1944\n",
      "Epoch [12/50], Step [200/600], Loss: 0.2729\n",
      "Epoch [12/50], Step [300/600], Loss: 0.2612\n",
      "Epoch [12/50], Step [400/600], Loss: 0.2348\n",
      "Epoch [12/50], Step [500/600], Loss: 0.2096\n",
      "Epoch [12/50], Step [600/600], Loss: 0.1923\n",
      "Epoch [13/50], Step [100/600], Loss: 0.2045\n",
      "Epoch [13/50], Step [200/600], Loss: 0.2153\n",
      "Epoch [13/50], Step [300/600], Loss: 0.2571\n",
      "Epoch [13/50], Step [400/600], Loss: 0.1276\n",
      "Epoch [13/50], Step [500/600], Loss: 0.1461\n",
      "Epoch [13/50], Step [600/600], Loss: 0.0960\n",
      "Epoch [14/50], Step [100/600], Loss: 0.1301\n",
      "Epoch [14/50], Step [200/600], Loss: 0.1983\n",
      "Epoch [14/50], Step [300/600], Loss: 0.2357\n",
      "Epoch [14/50], Step [400/600], Loss: 0.2029\n",
      "Epoch [14/50], Step [500/600], Loss: 0.2568\n",
      "Epoch [14/50], Step [600/600], Loss: 0.1143\n",
      "Epoch [15/50], Step [100/600], Loss: 0.0778\n",
      "Epoch [15/50], Step [200/600], Loss: 0.2280\n",
      "Epoch [15/50], Step [300/600], Loss: 0.1534\n",
      "Epoch [15/50], Step [400/600], Loss: 0.0994\n",
      "Epoch [15/50], Step [500/600], Loss: 0.1530\n",
      "Epoch [15/50], Step [600/600], Loss: 0.1575\n",
      "Epoch [16/50], Step [100/600], Loss: 0.2341\n",
      "Epoch [16/50], Step [200/600], Loss: 0.3153\n",
      "Epoch [16/50], Step [300/600], Loss: 0.1157\n",
      "Epoch [16/50], Step [400/600], Loss: 0.1798\n",
      "Epoch [16/50], Step [500/600], Loss: 0.1094\n",
      "Epoch [16/50], Step [600/600], Loss: 0.1270\n",
      "Epoch [17/50], Step [100/600], Loss: 0.1730\n",
      "Epoch [17/50], Step [200/600], Loss: 0.1208\n",
      "Epoch [17/50], Step [300/600], Loss: 0.2222\n",
      "Epoch [17/50], Step [400/600], Loss: 0.1059\n",
      "Epoch [17/50], Step [500/600], Loss: 0.1065\n",
      "Epoch [17/50], Step [600/600], Loss: 0.1668\n",
      "Epoch [18/50], Step [100/600], Loss: 0.0792\n",
      "Epoch [18/50], Step [200/600], Loss: 0.0750\n",
      "Epoch [18/50], Step [300/600], Loss: 0.1564\n",
      "Epoch [18/50], Step [400/600], Loss: 0.1470\n",
      "Epoch [18/50], Step [500/600], Loss: 0.1384\n",
      "Epoch [18/50], Step [600/600], Loss: 0.1113\n",
      "Epoch [19/50], Step [100/600], Loss: 0.1249\n",
      "Epoch [19/50], Step [200/600], Loss: 0.1328\n",
      "Epoch [19/50], Step [300/600], Loss: 0.2309\n",
      "Epoch [19/50], Step [400/600], Loss: 0.1254\n",
      "Epoch [19/50], Step [500/600], Loss: 0.1231\n",
      "Epoch [19/50], Step [600/600], Loss: 0.1532\n",
      "Epoch [20/50], Step [100/600], Loss: 0.1831\n",
      "Epoch [20/50], Step [200/600], Loss: 0.1716\n",
      "Epoch [20/50], Step [300/600], Loss: 0.1254\n",
      "Epoch [20/50], Step [400/600], Loss: 0.1560\n",
      "Epoch [20/50], Step [500/600], Loss: 0.0958\n",
      "Epoch [20/50], Step [600/600], Loss: 0.1410\n",
      "Epoch [21/50], Step [100/600], Loss: 0.1170\n",
      "Epoch [21/50], Step [200/600], Loss: 0.1708\n",
      "Epoch [21/50], Step [300/600], Loss: 0.0960\n",
      "Epoch [21/50], Step [400/600], Loss: 0.1172\n",
      "Epoch [21/50], Step [500/600], Loss: 0.1693\n",
      "Epoch [21/50], Step [600/600], Loss: 0.0989\n",
      "Epoch [22/50], Step [100/600], Loss: 0.2121\n",
      "Epoch [22/50], Step [200/600], Loss: 0.1933\n",
      "Epoch [22/50], Step [300/600], Loss: 0.1542\n",
      "Epoch [22/50], Step [400/600], Loss: 0.0370\n",
      "Epoch [22/50], Step [500/600], Loss: 0.2114\n",
      "Epoch [22/50], Step [600/600], Loss: 0.1762\n",
      "Epoch [23/50], Step [100/600], Loss: 0.1796\n",
      "Epoch [23/50], Step [200/600], Loss: 0.1972\n",
      "Epoch [23/50], Step [300/600], Loss: 0.1847\n",
      "Epoch [23/50], Step [400/600], Loss: 0.1352\n",
      "Epoch [23/50], Step [500/600], Loss: 0.1200\n",
      "Epoch [23/50], Step [600/600], Loss: 0.1869\n",
      "Epoch [24/50], Step [100/600], Loss: 0.2028\n",
      "Epoch [24/50], Step [200/600], Loss: 0.1056\n",
      "Epoch [24/50], Step [300/600], Loss: 0.0681\n",
      "Epoch [24/50], Step [400/600], Loss: 0.1330\n",
      "Epoch [24/50], Step [500/600], Loss: 0.2149\n",
      "Epoch [24/50], Step [600/600], Loss: 0.0691\n",
      "Epoch [25/50], Step [100/600], Loss: 0.1119\n",
      "Epoch [25/50], Step [200/600], Loss: 0.0898\n",
      "Epoch [25/50], Step [300/600], Loss: 0.1789\n",
      "Epoch [25/50], Step [400/600], Loss: 0.1564\n",
      "Epoch [25/50], Step [500/600], Loss: 0.1086\n",
      "Epoch [25/50], Step [600/600], Loss: 0.1079\n",
      "Epoch [26/50], Step [100/600], Loss: 0.1367\n",
      "Epoch [26/50], Step [200/600], Loss: 0.1017\n",
      "Epoch [26/50], Step [300/600], Loss: 0.0583\n",
      "Epoch [26/50], Step [400/600], Loss: 0.1085\n",
      "Epoch [26/50], Step [500/600], Loss: 0.0835\n",
      "Epoch [26/50], Step [600/600], Loss: 0.0655\n",
      "Epoch [27/50], Step [100/600], Loss: 0.1309\n",
      "Epoch [27/50], Step [200/600], Loss: 0.0697\n",
      "Epoch [27/50], Step [300/600], Loss: 0.0973\n",
      "Epoch [27/50], Step [400/600], Loss: 0.2168\n",
      "Epoch [27/50], Step [500/600], Loss: 0.0665\n",
      "Epoch [27/50], Step [600/600], Loss: 0.0490\n",
      "Epoch [28/50], Step [100/600], Loss: 0.1021\n",
      "Epoch [28/50], Step [200/600], Loss: 0.0887\n",
      "Epoch [28/50], Step [300/600], Loss: 0.1697\n",
      "Epoch [28/50], Step [400/600], Loss: 0.0887\n",
      "Epoch [28/50], Step [500/600], Loss: 0.0973\n",
      "Epoch [28/50], Step [600/600], Loss: 0.1515\n",
      "Epoch [29/50], Step [100/600], Loss: 0.0988\n",
      "Epoch [29/50], Step [200/600], Loss: 0.0665\n",
      "Epoch [29/50], Step [300/600], Loss: 0.0767\n",
      "Epoch [29/50], Step [400/600], Loss: 0.0967\n",
      "Epoch [29/50], Step [500/600], Loss: 0.1063\n",
      "Epoch [29/50], Step [600/600], Loss: 0.1345\n",
      "Epoch [30/50], Step [100/600], Loss: 0.1776\n",
      "Epoch [30/50], Step [200/600], Loss: 0.1153\n",
      "Epoch [30/50], Step [300/600], Loss: 0.1792\n",
      "Epoch [30/50], Step [400/600], Loss: 0.0357\n",
      "Epoch [30/50], Step [500/600], Loss: 0.0769\n",
      "Epoch [30/50], Step [600/600], Loss: 0.1088\n",
      "Epoch [31/50], Step [100/600], Loss: 0.2316\n",
      "Epoch [31/50], Step [200/600], Loss: 0.0469\n",
      "Epoch [31/50], Step [300/600], Loss: 0.1772\n",
      "Epoch [31/50], Step [400/600], Loss: 0.0745\n",
      "Epoch [31/50], Step [500/600], Loss: 0.3137\n",
      "Epoch [31/50], Step [600/600], Loss: 0.1813\n",
      "Epoch [32/50], Step [100/600], Loss: 0.1410\n",
      "Epoch [32/50], Step [200/600], Loss: 0.0909\n",
      "Epoch [32/50], Step [300/600], Loss: 0.0860\n",
      "Epoch [32/50], Step [400/600], Loss: 0.0961\n",
      "Epoch [32/50], Step [500/600], Loss: 0.1291\n",
      "Epoch [32/50], Step [600/600], Loss: 0.1342\n",
      "Epoch [33/50], Step [100/600], Loss: 0.0628\n",
      "Epoch [33/50], Step [200/600], Loss: 0.0859\n",
      "Epoch [33/50], Step [300/600], Loss: 0.2015\n",
      "Epoch [33/50], Step [400/600], Loss: 0.1285\n",
      "Epoch [33/50], Step [500/600], Loss: 0.1188\n",
      "Epoch [33/50], Step [600/600], Loss: 0.1052\n",
      "Epoch [34/50], Step [100/600], Loss: 0.0783\n",
      "Epoch [34/50], Step [200/600], Loss: 0.0775\n",
      "Epoch [34/50], Step [300/600], Loss: 0.1966\n",
      "Epoch [34/50], Step [400/600], Loss: 0.1047\n",
      "Epoch [34/50], Step [500/600], Loss: 0.1049\n",
      "Epoch [34/50], Step [600/600], Loss: 0.1219\n",
      "Epoch [35/50], Step [100/600], Loss: 0.0644\n",
      "Epoch [35/50], Step [200/600], Loss: 0.0899\n",
      "Epoch [35/50], Step [300/600], Loss: 0.0719\n",
      "Epoch [35/50], Step [400/600], Loss: 0.0986\n",
      "Epoch [35/50], Step [500/600], Loss: 0.1017\n",
      "Epoch [35/50], Step [600/600], Loss: 0.0695\n",
      "Epoch [36/50], Step [100/600], Loss: 0.1601\n",
      "Epoch [36/50], Step [200/600], Loss: 0.0686\n",
      "Epoch [36/50], Step [300/600], Loss: 0.0554\n",
      "Epoch [36/50], Step [400/600], Loss: 0.0849\n",
      "Epoch [36/50], Step [500/600], Loss: 0.0959\n",
      "Epoch [36/50], Step [600/600], Loss: 0.0827\n",
      "Epoch [37/50], Step [100/600], Loss: 0.0648\n",
      "Epoch [37/50], Step [200/600], Loss: 0.0598\n",
      "Epoch [37/50], Step [300/600], Loss: 0.1699\n",
      "Epoch [37/50], Step [400/600], Loss: 0.1679\n",
      "Epoch [37/50], Step [500/600], Loss: 0.0778\n",
      "Epoch [37/50], Step [600/600], Loss: 0.0744\n",
      "Epoch [38/50], Step [100/600], Loss: 0.0922\n",
      "Epoch [38/50], Step [200/600], Loss: 0.1702\n",
      "Epoch [38/50], Step [300/600], Loss: 0.0321\n",
      "Epoch [38/50], Step [400/600], Loss: 0.1171\n",
      "Epoch [38/50], Step [500/600], Loss: 0.0424\n",
      "Epoch [38/50], Step [600/600], Loss: 0.0572\n",
      "Epoch [39/50], Step [100/600], Loss: 0.0660\n",
      "Epoch [39/50], Step [200/600], Loss: 0.1007\n",
      "Epoch [39/50], Step [300/600], Loss: 0.0823\n",
      "Epoch [39/50], Step [400/600], Loss: 0.0980\n",
      "Epoch [39/50], Step [500/600], Loss: 0.0399\n",
      "Epoch [39/50], Step [600/600], Loss: 0.0506\n",
      "Epoch [40/50], Step [100/600], Loss: 0.1077\n",
      "Epoch [40/50], Step [200/600], Loss: 0.1081\n",
      "Epoch [40/50], Step [300/600], Loss: 0.0727\n",
      "Epoch [40/50], Step [400/600], Loss: 0.0594\n",
      "Epoch [40/50], Step [500/600], Loss: 0.0453\n",
      "Epoch [40/50], Step [600/600], Loss: 0.0812\n",
      "Epoch [41/50], Step [100/600], Loss: 0.0228\n",
      "Epoch [41/50], Step [200/600], Loss: 0.0629\n",
      "Epoch [41/50], Step [300/600], Loss: 0.1341\n",
      "Epoch [41/50], Step [400/600], Loss: 0.0413\n",
      "Epoch [41/50], Step [500/600], Loss: 0.0597\n",
      "Epoch [41/50], Step [600/600], Loss: 0.0790\n",
      "Epoch [42/50], Step [100/600], Loss: 0.0728\n",
      "Epoch [42/50], Step [200/600], Loss: 0.2222\n",
      "Epoch [42/50], Step [300/600], Loss: 0.0881\n",
      "Epoch [42/50], Step [400/600], Loss: 0.0906\n",
      "Epoch [42/50], Step [500/600], Loss: 0.0732\n",
      "Epoch [42/50], Step [600/600], Loss: 0.0315\n",
      "Epoch [43/50], Step [100/600], Loss: 0.0909\n",
      "Epoch [43/50], Step [200/600], Loss: 0.0887\n",
      "Epoch [43/50], Step [300/600], Loss: 0.0373\n",
      "Epoch [43/50], Step [400/600], Loss: 0.1207\n",
      "Epoch [43/50], Step [500/600], Loss: 0.0674\n",
      "Epoch [43/50], Step [600/600], Loss: 0.0538\n",
      "Epoch [44/50], Step [100/600], Loss: 0.0586\n",
      "Epoch [44/50], Step [200/600], Loss: 0.0969\n",
      "Epoch [44/50], Step [300/600], Loss: 0.0529\n",
      "Epoch [44/50], Step [400/600], Loss: 0.1477\n",
      "Epoch [44/50], Step [500/600], Loss: 0.0717\n",
      "Epoch [44/50], Step [600/600], Loss: 0.0825\n",
      "Epoch [45/50], Step [100/600], Loss: 0.0925\n",
      "Epoch [45/50], Step [200/600], Loss: 0.0544\n",
      "Epoch [45/50], Step [300/600], Loss: 0.0457\n",
      "Epoch [45/50], Step [400/600], Loss: 0.0511\n",
      "Epoch [45/50], Step [500/600], Loss: 0.2159\n",
      "Epoch [45/50], Step [600/600], Loss: 0.0662\n",
      "Epoch [46/50], Step [100/600], Loss: 0.0671\n",
      "Epoch [46/50], Step [200/600], Loss: 0.0592\n",
      "Epoch [46/50], Step [300/600], Loss: 0.0379\n",
      "Epoch [46/50], Step [400/600], Loss: 0.0565\n",
      "Epoch [46/50], Step [500/600], Loss: 0.1197\n",
      "Epoch [46/50], Step [600/600], Loss: 0.0970\n",
      "Epoch [47/50], Step [100/600], Loss: 0.0873\n",
      "Epoch [47/50], Step [200/600], Loss: 0.0796\n",
      "Epoch [47/50], Step [300/600], Loss: 0.0419\n",
      "Epoch [47/50], Step [400/600], Loss: 0.0714\n",
      "Epoch [47/50], Step [500/600], Loss: 0.0540\n",
      "Epoch [47/50], Step [600/600], Loss: 0.1835\n",
      "Epoch [48/50], Step [100/600], Loss: 0.0970\n",
      "Epoch [48/50], Step [200/600], Loss: 0.0479\n",
      "Epoch [48/50], Step [300/600], Loss: 0.0106\n",
      "Epoch [48/50], Step [400/600], Loss: 0.0456\n",
      "Epoch [48/50], Step [500/600], Loss: 0.0423\n",
      "Epoch [48/50], Step [600/600], Loss: 0.1058\n",
      "Epoch [49/50], Step [100/600], Loss: 0.1480\n",
      "Epoch [49/50], Step [200/600], Loss: 0.1205\n",
      "Epoch [49/50], Step [300/600], Loss: 0.0904\n",
      "Epoch [49/50], Step [400/600], Loss: 0.1216\n",
      "Epoch [49/50], Step [500/600], Loss: 0.0291\n",
      "Epoch [49/50], Step [600/600], Loss: 0.0723\n",
      "Epoch [50/50], Step [100/600], Loss: 0.1614\n",
      "Epoch [50/50], Step [200/600], Loss: 0.1037\n",
      "Epoch [50/50], Step [300/600], Loss: 0.1010\n",
      "Epoch [50/50], Step [400/600], Loss: 0.0303\n",
      "Epoch [50/50], Step [500/600], Loss: 0.0660\n",
      "Epoch [50/50], Step [600/600], Loss: 0.0677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x149a3eed1820>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracies1, size1 = runModel(1)\n",
    "plt.plot(size1, accuracies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.6321,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5951, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.6321,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5951, 0.0000]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 0.3679,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.4049, 1.0000]]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 0.3679,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.4049, 1.0000]]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 0.3679,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.5951, 0.0000]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x100 and 50x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracies2, size2 \u001b[38;5;241m=\u001b[39m \u001b[43mrunModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(size2, accuracies2)\n",
      "Cell \u001b[0;32mIn[47], line 32\u001b[0m, in \u001b[0;36mrunModel\u001b[0;34m(count)\u001b[0m\n\u001b[1;32m     29\u001b[0m labels \u001b[38;5;241m=\u001b[39m Variable(labels)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 59\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m out \u001b[38;5;241m=\u001b[39m bernoulli_reparam(sample_poisson_relaxed(torch\u001b[38;5;241m.\u001b[39msquare(out), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# out = self.relu(out)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x100 and 50x10)"
     ]
    }
   ],
   "source": [
    "accuracies2, size2 = runModel(2)\n",
    "plt.plot(size2, accuracies2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.6321, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.6321, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.3679, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.3679, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.6321, 0.0000, 0.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x100 and 50x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracies5, size5 \u001b[38;5;241m=\u001b[39m \u001b[43mrunModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(size5, accuracies5)\n",
      "Cell \u001b[0;32mIn[47], line 32\u001b[0m, in \u001b[0;36mrunModel\u001b[0;34m(count)\u001b[0m\n\u001b[1;32m     29\u001b[0m labels \u001b[38;5;241m=\u001b[39m Variable(labels)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 59\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m out \u001b[38;5;241m=\u001b[39m bernoulli_reparam(sample_poisson_relaxed(torch\u001b[38;5;241m.\u001b[39msquare(out), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# out = self.relu(out)\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/clusterenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x100 and 50x10)"
     ]
    }
   ],
   "source": [
    "accuracies5, size5 = runModel(5)\n",
    "plt.plot(size5, accuracies5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies100, size100 = runModel(10)\n",
    "plt.plot(size100, accuracies100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Number of Hidden Neurons')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Numerical Simulation')\n",
    "plt.plot(size1, accuracies1)\n",
    "plt.plot(size2, accuracies2)\n",
    "# plt.plot(size5, accuracies5)\n",
    "# plt.plot(size100, accuracies100)\n",
    "# plt.legend([\"1-shot\", \"2-shot\", \"5-shot\", \"10-shot\"], loc=\"lower right\")\n",
    "plt.legend([\"1-shot\", \"2-shot\"],loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Pytorch MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
