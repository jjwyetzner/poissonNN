{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "bGU6NwlsXFSt"
   },
   "outputs": [],
   "source": [
    "#@title Import Dependencies\n",
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "_bNfVLRUYqZA"
   },
   "outputs": [],
   "source": [
    "#@title Define Hyperparameters\n",
    "\n",
    "input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
    "hidden_size = 500 # number of nodes at hidden layer\n",
    "num_classes = 10 # number of output classes discrete range [0,9]\n",
    "num_epochs = 100 # number of times which the entire dataset is passed throughout the model\n",
    "batch_size = 100 # the size of input data took for one iteration\n",
    "lr = 1e-2 # size of step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCsBCXMwbpH5"
   },
   "outputs": [],
   "source": [
    "#@title Downloading MNIST data\n",
    "\n",
    "train_data = dsets.MNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "test_data = dsets.MNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rfDPBdnYgfGp"
   },
   "outputs": [],
   "source": [
    "#@title Loading the data\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                             batch_size = batch_size,\n",
    "                                             shuffle = True)\n",
    "\n",
    "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                      batch_size = batch_size, \n",
    "                                      shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class PhotonCountingP(nn.Module):\n",
    "    \"\"\" The probability of 1 photon in photon counting \n",
    "        (also the expectation value) with mean flux x \"\"\"\n",
    "    def __init__(self):\n",
    "        super(PhotonCountingP, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 1.-torch.exp(torch.abs(x)*-1.)\n",
    "    \n",
    "class BernoulliFunctionST(Function):\n",
    "    \"\"\" The 'Straight Through' stochastic Bernoulli activation\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "\n",
    "        return torch.bernoulli(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        return grad_output\n",
    "\n",
    "class PoissonFunctionST(Function):\n",
    "    \"\"\" The 'Straight Through' stochastic Poisson activation\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "\n",
    "        return torch.poisson(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "\n",
    "        return grad_output\n",
    "    \n",
    "PoissonST = PoissonFunctionST.apply    \n",
    "BernoulliST = BernoulliFunctionST.apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fL-YXTvghaz_"
   },
   "outputs": [],
   "source": [
    "#for photonActivation\n",
    "\n",
    "# class Net(nn.Module):\n",
    "#   def __init__(self, input_size, hidden_size, num_classes, scale = 1, slope = 1, repeat = 5):\n",
    "#     super(Net,self).__init__()\n",
    "#     self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#     self.relu = nn.ReLU()\n",
    "#     self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "#     self.act = PhotonCountingP()\n",
    "#     self.sampler = BernoulliST\n",
    "#     self.scale = scale\n",
    "#     self.slope = slope\n",
    "#     self.repeat = repeat\n",
    "  \n",
    "#   def forward(self,x):\n",
    "#     out = self.fc1(x)\n",
    "#     # out = self.relu(out)\n",
    "#     probs = self.act(self.slope * out)\n",
    "#     out = self.sampler(probs.unsqueeze(0).repeat((self.repeat,)+(1,)*len(probs.shape))).mean(axis=0)*torch.sign(out)\n",
    "#     out = self.fc2(out)\n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for poisson\n",
    "from torch.distributions import Exponential\n",
    "\n",
    "def sample_poisson_relaxed(lmbd, num_samples=5, temperature = 0.01):\n",
    "    try:\n",
    "        sampler = torch.empty(num_samples)\n",
    "        z = Exponential(lmbd).rsample(sampler.size())\n",
    "        t = torch.cumsum(z, 0)\n",
    "        relaxed_indicator = torch.sigmoid((1.0 - t) / temperature)\n",
    "        N = torch.cumsum(relaxed_indicator, 0)\n",
    "        return N[-1]\n",
    "    except Exception:\n",
    "        return lmbd\n",
    "    \n",
    "class Net(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes, scale = 1, slope = 1, repeat = 5):\n",
    "    super(Net,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    self.repeat = repeat\n",
    "  \n",
    "  def forward(self,x):\n",
    "    out = self.fc1(x)\n",
    "    out = sample_poisson_relaxed(torch.square(out), self.repeat)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3EPEqbjjfAT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda:0\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "#@title Build the model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(1)\n",
    "else:\n",
    "    print(0)\n",
    "print(torch.device(0))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u75Xa5VckuTH"
   },
   "outputs": [],
   "source": [
    "#@title Training the model\n",
    "\n",
    "\n",
    "def runModel(count):\n",
    "  accuracies = []\n",
    "  # photonCounts = []\n",
    "  hiddenLayer = []\n",
    "\n",
    "  # finalaccuracies = []\n",
    "  # finalCounts = []\n",
    "\n",
    "  # for j in [50,100,200,300,400]:\n",
    "  for j in [400]:\n",
    "\n",
    "    hidden_size = j\n",
    "    # count = j + 1\n",
    "    net = Net(input_size, hidden_size, num_classes, repeat= count)\n",
    "    if torch.cuda.is_available():\n",
    "       net.cuda()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam( net.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr)\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.99, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "    max = 0\n",
    "    for epoch in range(num_epochs):\n",
    "      for i ,(images,labels) in enumerate(train_gen):\n",
    "        images = Variable(images.view(-1,28*28)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "          print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                    %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))\n",
    "        # if (i+1) % 100 == 0:\n",
    "        #   print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Accuracy: %.1f'\n",
    "        #             %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item(), accuracy))\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for images,labels in test_gen:\n",
    "        images = Variable(images.view(-1,28*28)).cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        output = net(images)\n",
    "        _, predicted = torch.max(output,1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        total += labels.size(0)\n",
    "\n",
    "      accuracy = (100*correct)/(total+1)\n",
    "      if max < accuracy.item():\n",
    "        max = accuracy.item()\n",
    "\n",
    "    accuracies.append(max)\n",
    "    hiddenLayer.append(hidden_size)\n",
    "        # photonCounts.append(count)\n",
    "    # finalaccuracies.append(accuracy)\n",
    "    # finalCounts.append(count)\n",
    "  return accuracies, hiddenLayer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(accuracies)):\n",
    "#     accuracies[i] = accuracies[i].item()\n",
    "\n",
    "# for i in range(len(finalaccuracies)):\n",
    "#     finalaccuracies[i] = finalaccuracies[i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTPvMW5jHB9X"
   },
   "outputs": [],
   "source": [
    "#@title Evaluating the accuracy of the model\n",
    "\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for images,labels in test_gen:\n",
    "#   images = Variable(images.view(-1,28*28))\n",
    "#   labels = labels\n",
    "  \n",
    "#   output = net(images)\n",
    "#   _, predicted = torch.max(output,1)\n",
    "#   correct += (predicted == labels).sum()\n",
    "#   total += labels.size(0)\n",
    "\n",
    "# print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# print(photonCounts)\n",
    "# print(accuracies)\n",
    "# plt.plot(photonCounts, accuracies, 'o')\n",
    "# plt.plot(finalCounts, finalaccuracies)\n",
    "# plt.xlabel('Number of shots')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.title('Shot number vs. Accuracy after 10 epochs')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/600], Loss: 2.2167\n",
      "Epoch [1/100], Step [200/600], Loss: 2.1762\n",
      "Epoch [1/100], Step [300/600], Loss: 2.0037\n",
      "Epoch [1/100], Step [400/600], Loss: 1.8270\n",
      "Epoch [1/100], Step [500/600], Loss: 1.6232\n",
      "Epoch [1/100], Step [600/600], Loss: 1.4565\n",
      "Epoch [2/100], Step [100/600], Loss: 1.2644\n",
      "Epoch [2/100], Step [200/600], Loss: 1.0234\n",
      "Epoch [2/100], Step [300/600], Loss: 0.8530\n",
      "Epoch [2/100], Step [400/600], Loss: 0.8163\n",
      "Epoch [2/100], Step [500/600], Loss: 0.8663\n",
      "Epoch [2/100], Step [600/600], Loss: 0.8172\n",
      "Epoch [3/100], Step [100/600], Loss: 0.7519\n",
      "Epoch [3/100], Step [200/600], Loss: 0.6422\n",
      "Epoch [3/100], Step [300/600], Loss: 0.6926\n",
      "Epoch [3/100], Step [400/600], Loss: 0.6838\n",
      "Epoch [3/100], Step [500/600], Loss: 0.5081\n",
      "Epoch [3/100], Step [600/600], Loss: 0.5467\n",
      "Epoch [4/100], Step [100/600], Loss: 0.4932\n",
      "Epoch [4/100], Step [200/600], Loss: 0.5218\n",
      "Epoch [4/100], Step [300/600], Loss: 0.4851\n",
      "Epoch [4/100], Step [400/600], Loss: 0.6095\n",
      "Epoch [4/100], Step [500/600], Loss: 0.5371\n",
      "Epoch [4/100], Step [600/600], Loss: 0.5343\n",
      "Epoch [5/100], Step [100/600], Loss: 0.6830\n",
      "Epoch [5/100], Step [200/600], Loss: 0.5140\n",
      "Epoch [5/100], Step [300/600], Loss: 0.4814\n",
      "Epoch [5/100], Step [400/600], Loss: 0.5555\n",
      "Epoch [5/100], Step [500/600], Loss: 0.5059\n",
      "Epoch [5/100], Step [600/600], Loss: 0.4633\n",
      "Epoch [6/100], Step [100/600], Loss: 0.4906\n",
      "Epoch [6/100], Step [200/600], Loss: 0.5459\n",
      "Epoch [6/100], Step [300/600], Loss: 0.4206\n",
      "Epoch [6/100], Step [400/600], Loss: 0.5065\n",
      "Epoch [6/100], Step [500/600], Loss: 0.4081\n",
      "Epoch [6/100], Step [600/600], Loss: 0.5464\n",
      "Epoch [7/100], Step [100/600], Loss: 0.4009\n",
      "Epoch [7/100], Step [200/600], Loss: 0.3579\n",
      "Epoch [7/100], Step [300/600], Loss: 0.5159\n",
      "Epoch [7/100], Step [400/600], Loss: 0.4607\n",
      "Epoch [7/100], Step [500/600], Loss: 0.3968\n",
      "Epoch [7/100], Step [600/600], Loss: 0.3334\n",
      "Epoch [8/100], Step [100/600], Loss: 0.4068\n",
      "Epoch [8/100], Step [200/600], Loss: 0.3252\n",
      "Epoch [8/100], Step [300/600], Loss: 0.4765\n",
      "Epoch [8/100], Step [400/600], Loss: 0.3702\n",
      "Epoch [8/100], Step [500/600], Loss: 0.2990\n",
      "Epoch [8/100], Step [600/600], Loss: 0.3876\n",
      "Epoch [9/100], Step [100/600], Loss: 0.4009\n",
      "Epoch [9/100], Step [200/600], Loss: 0.4637\n",
      "Epoch [9/100], Step [300/600], Loss: 0.2847\n",
      "Epoch [9/100], Step [400/600], Loss: 0.4603\n",
      "Epoch [9/100], Step [500/600], Loss: 0.3320\n",
      "Epoch [9/100], Step [600/600], Loss: 0.3624\n",
      "Epoch [10/100], Step [100/600], Loss: 0.3193\n",
      "Epoch [10/100], Step [200/600], Loss: 0.3029\n",
      "Epoch [10/100], Step [300/600], Loss: 0.3701\n",
      "Epoch [10/100], Step [400/600], Loss: 0.3386\n",
      "Epoch [10/100], Step [500/600], Loss: 0.4124\n",
      "Epoch [10/100], Step [600/600], Loss: 0.3559\n",
      "Epoch [11/100], Step [100/600], Loss: 0.3480\n",
      "Epoch [11/100], Step [200/600], Loss: 0.2549\n",
      "Epoch [11/100], Step [300/600], Loss: 0.3045\n",
      "Epoch [11/100], Step [400/600], Loss: 0.4237\n",
      "Epoch [11/100], Step [500/600], Loss: 0.3621\n",
      "Epoch [11/100], Step [600/600], Loss: 0.3992\n",
      "Epoch [12/100], Step [100/600], Loss: 0.3027\n",
      "Epoch [12/100], Step [200/600], Loss: 0.2035\n",
      "Epoch [12/100], Step [300/600], Loss: 0.2463\n",
      "Epoch [12/100], Step [400/600], Loss: 0.3129\n",
      "Epoch [12/100], Step [500/600], Loss: 0.2702\n",
      "Epoch [12/100], Step [600/600], Loss: 0.2882\n",
      "Epoch [13/100], Step [100/600], Loss: 0.3318\n",
      "Epoch [13/100], Step [200/600], Loss: 0.3006\n",
      "Epoch [13/100], Step [300/600], Loss: 0.4090\n",
      "Epoch [13/100], Step [400/600], Loss: 0.3158\n",
      "Epoch [13/100], Step [500/600], Loss: 0.2881\n",
      "Epoch [13/100], Step [600/600], Loss: 0.2329\n",
      "Epoch [14/100], Step [100/600], Loss: 0.2622\n",
      "Epoch [14/100], Step [200/600], Loss: 0.3078\n",
      "Epoch [14/100], Step [300/600], Loss: 0.2700\n",
      "Epoch [14/100], Step [400/600], Loss: 0.2297\n",
      "Epoch [14/100], Step [500/600], Loss: 0.2420\n",
      "Epoch [14/100], Step [600/600], Loss: 0.2045\n",
      "Epoch [15/100], Step [100/600], Loss: 0.3247\n",
      "Epoch [15/100], Step [200/600], Loss: 0.3521\n",
      "Epoch [15/100], Step [300/600], Loss: 0.2232\n",
      "Epoch [15/100], Step [400/600], Loss: 0.3155\n",
      "Epoch [15/100], Step [500/600], Loss: 0.2449\n",
      "Epoch [15/100], Step [600/600], Loss: 0.1915\n",
      "Epoch [16/100], Step [100/600], Loss: 0.2205\n",
      "Epoch [16/100], Step [200/600], Loss: 0.2636\n",
      "Epoch [16/100], Step [300/600], Loss: 0.1937\n",
      "Epoch [16/100], Step [400/600], Loss: 0.3552\n",
      "Epoch [16/100], Step [500/600], Loss: 0.2820\n",
      "Epoch [16/100], Step [600/600], Loss: 0.2040\n",
      "Epoch [17/100], Step [100/600], Loss: 0.2587\n",
      "Epoch [17/100], Step [200/600], Loss: 0.2342\n",
      "Epoch [17/100], Step [300/600], Loss: 0.2649\n",
      "Epoch [17/100], Step [400/600], Loss: 0.3694\n",
      "Epoch [17/100], Step [500/600], Loss: 0.2095\n",
      "Epoch [17/100], Step [600/600], Loss: 0.3302\n",
      "Epoch [18/100], Step [100/600], Loss: 0.3022\n",
      "Epoch [18/100], Step [200/600], Loss: 0.2143\n",
      "Epoch [18/100], Step [300/600], Loss: 0.3318\n",
      "Epoch [18/100], Step [400/600], Loss: 0.2011\n",
      "Epoch [18/100], Step [500/600], Loss: 0.3039\n",
      "Epoch [18/100], Step [600/600], Loss: 0.2462\n",
      "Epoch [19/100], Step [100/600], Loss: 0.2804\n",
      "Epoch [19/100], Step [200/600], Loss: 0.2344\n",
      "Epoch [19/100], Step [300/600], Loss: 0.3557\n",
      "Epoch [19/100], Step [400/600], Loss: 0.2282\n",
      "Epoch [19/100], Step [500/600], Loss: 0.2074\n",
      "Epoch [19/100], Step [600/600], Loss: 0.2581\n",
      "Epoch [20/100], Step [100/600], Loss: 0.2807\n",
      "Epoch [20/100], Step [200/600], Loss: 0.2273\n",
      "Epoch [20/100], Step [300/600], Loss: 0.3039\n",
      "Epoch [20/100], Step [400/600], Loss: 0.2255\n",
      "Epoch [20/100], Step [500/600], Loss: 0.4405\n",
      "Epoch [20/100], Step [600/600], Loss: 0.2437\n",
      "Epoch [21/100], Step [100/600], Loss: 0.1500\n",
      "Epoch [21/100], Step [200/600], Loss: 0.3398\n",
      "Epoch [21/100], Step [300/600], Loss: 0.2521\n",
      "Epoch [21/100], Step [400/600], Loss: 0.2548\n",
      "Epoch [21/100], Step [500/600], Loss: 0.1056\n",
      "Epoch [21/100], Step [600/600], Loss: 0.2211\n",
      "Epoch [22/100], Step [100/600], Loss: 0.2021\n",
      "Epoch [22/100], Step [200/600], Loss: 0.2968\n",
      "Epoch [22/100], Step [300/600], Loss: 0.3113\n",
      "Epoch [22/100], Step [400/600], Loss: 0.1103\n",
      "Epoch [22/100], Step [500/600], Loss: 0.1663\n",
      "Epoch [22/100], Step [600/600], Loss: 0.1277\n",
      "Epoch [23/100], Step [100/600], Loss: 0.3545\n",
      "Epoch [23/100], Step [200/600], Loss: 0.3357\n",
      "Epoch [23/100], Step [300/600], Loss: 0.1175\n",
      "Epoch [23/100], Step [400/600], Loss: 0.1210\n",
      "Epoch [23/100], Step [500/600], Loss: 0.3158\n",
      "Epoch [23/100], Step [600/600], Loss: 0.2604\n",
      "Epoch [24/100], Step [100/600], Loss: 0.1414\n",
      "Epoch [24/100], Step [200/600], Loss: 0.2200\n",
      "Epoch [24/100], Step [300/600], Loss: 0.2702\n",
      "Epoch [24/100], Step [400/600], Loss: 0.1866\n",
      "Epoch [24/100], Step [500/600], Loss: 0.2945\n",
      "Epoch [24/100], Step [600/600], Loss: 0.2255\n",
      "Epoch [25/100], Step [100/600], Loss: 0.2646\n",
      "Epoch [25/100], Step [200/600], Loss: 0.2241\n",
      "Epoch [25/100], Step [300/600], Loss: 0.1563\n",
      "Epoch [25/100], Step [400/600], Loss: 0.1424\n",
      "Epoch [25/100], Step [500/600], Loss: 0.2442\n",
      "Epoch [25/100], Step [600/600], Loss: 0.1402\n",
      "Epoch [26/100], Step [100/600], Loss: 0.3066\n",
      "Epoch [26/100], Step [200/600], Loss: 0.3300\n",
      "Epoch [26/100], Step [300/600], Loss: 0.1566\n",
      "Epoch [26/100], Step [400/600], Loss: 0.1241\n",
      "Epoch [26/100], Step [500/600], Loss: 0.2133\n",
      "Epoch [26/100], Step [600/600], Loss: 0.2599\n",
      "Epoch [27/100], Step [100/600], Loss: 0.2331\n",
      "Epoch [27/100], Step [200/600], Loss: 0.2531\n",
      "Epoch [27/100], Step [300/600], Loss: 0.1665\n",
      "Epoch [27/100], Step [400/600], Loss: 0.3002\n",
      "Epoch [27/100], Step [500/600], Loss: 0.1896\n",
      "Epoch [27/100], Step [600/600], Loss: 0.1914\n",
      "Epoch [28/100], Step [100/600], Loss: 0.3906\n",
      "Epoch [28/100], Step [200/600], Loss: 0.2026\n",
      "Epoch [28/100], Step [300/600], Loss: 0.1360\n",
      "Epoch [28/100], Step [400/600], Loss: 0.2639\n",
      "Epoch [28/100], Step [500/600], Loss: 0.1810\n",
      "Epoch [28/100], Step [600/600], Loss: 0.1366\n",
      "Epoch [29/100], Step [100/600], Loss: 0.1367\n",
      "Epoch [29/100], Step [200/600], Loss: 0.1075\n",
      "Epoch [29/100], Step [300/600], Loss: 0.3066\n",
      "Epoch [29/100], Step [400/600], Loss: 0.1884\n",
      "Epoch [29/100], Step [500/600], Loss: 0.2531\n",
      "Epoch [29/100], Step [600/600], Loss: 0.1555\n",
      "Epoch [30/100], Step [100/600], Loss: 0.2255\n",
      "Epoch [30/100], Step [200/600], Loss: 0.1206\n",
      "Epoch [30/100], Step [300/600], Loss: 0.1770\n",
      "Epoch [30/100], Step [400/600], Loss: 0.2180\n",
      "Epoch [30/100], Step [500/600], Loss: 0.1535\n",
      "Epoch [30/100], Step [600/600], Loss: 0.1857\n",
      "Epoch [31/100], Step [100/600], Loss: 0.1207\n",
      "Epoch [31/100], Step [200/600], Loss: 0.2008\n",
      "Epoch [31/100], Step [300/600], Loss: 0.1723\n",
      "Epoch [31/100], Step [400/600], Loss: 0.2000\n",
      "Epoch [31/100], Step [500/600], Loss: 0.2124\n",
      "Epoch [31/100], Step [600/600], Loss: 0.1542\n",
      "Epoch [32/100], Step [100/600], Loss: 0.2279\n",
      "Epoch [32/100], Step [200/600], Loss: 0.2203\n",
      "Epoch [32/100], Step [300/600], Loss: 0.2329\n",
      "Epoch [32/100], Step [400/600], Loss: 0.1575\n",
      "Epoch [32/100], Step [500/600], Loss: 0.1150\n",
      "Epoch [32/100], Step [600/600], Loss: 0.1710\n",
      "Epoch [33/100], Step [100/600], Loss: 0.2548\n",
      "Epoch [33/100], Step [200/600], Loss: 0.1547\n",
      "Epoch [33/100], Step [300/600], Loss: 0.1366\n",
      "Epoch [33/100], Step [400/600], Loss: 0.1593\n",
      "Epoch [33/100], Step [500/600], Loss: 0.2657\n",
      "Epoch [33/100], Step [600/600], Loss: 0.2252\n",
      "Epoch [34/100], Step [100/600], Loss: 0.1712\n",
      "Epoch [34/100], Step [200/600], Loss: 0.1915\n",
      "Epoch [34/100], Step [300/600], Loss: 0.1488\n",
      "Epoch [34/100], Step [400/600], Loss: 0.3141\n",
      "Epoch [34/100], Step [500/600], Loss: 0.2214\n",
      "Epoch [34/100], Step [600/600], Loss: 0.1439\n",
      "Epoch [35/100], Step [100/600], Loss: 0.1103\n",
      "Epoch [35/100], Step [200/600], Loss: 0.1690\n",
      "Epoch [35/100], Step [300/600], Loss: 0.0634\n",
      "Epoch [35/100], Step [400/600], Loss: 0.0941\n",
      "Epoch [35/100], Step [500/600], Loss: 0.2056\n",
      "Epoch [35/100], Step [600/600], Loss: 0.1575\n",
      "Epoch [36/100], Step [100/600], Loss: 0.3224\n",
      "Epoch [36/100], Step [200/600], Loss: 0.1299\n",
      "Epoch [36/100], Step [300/600], Loss: 0.2329\n",
      "Epoch [36/100], Step [400/600], Loss: 0.1952\n",
      "Epoch [36/100], Step [500/600], Loss: 0.1892\n",
      "Epoch [36/100], Step [600/600], Loss: 0.2847\n",
      "Epoch [37/100], Step [100/600], Loss: 0.1790\n",
      "Epoch [37/100], Step [200/600], Loss: 0.4552\n",
      "Epoch [37/100], Step [300/600], Loss: 0.1261\n",
      "Epoch [37/100], Step [400/600], Loss: 0.2254\n",
      "Epoch [37/100], Step [500/600], Loss: 0.2134\n",
      "Epoch [37/100], Step [600/600], Loss: 0.1094\n",
      "Epoch [38/100], Step [100/600], Loss: 0.1630\n",
      "Epoch [38/100], Step [200/600], Loss: 0.2614\n",
      "Epoch [38/100], Step [300/600], Loss: 0.1113\n",
      "Epoch [38/100], Step [400/600], Loss: 0.2568\n",
      "Epoch [38/100], Step [500/600], Loss: 0.1694\n",
      "Epoch [38/100], Step [600/600], Loss: 0.3223\n",
      "Epoch [39/100], Step [100/600], Loss: 0.0964\n",
      "Epoch [39/100], Step [200/600], Loss: 0.1701\n",
      "Epoch [39/100], Step [300/600], Loss: 0.1457\n",
      "Epoch [39/100], Step [400/600], Loss: 0.1674\n",
      "Epoch [39/100], Step [500/600], Loss: 0.1118\n",
      "Epoch [39/100], Step [600/600], Loss: 0.2748\n",
      "Epoch [40/100], Step [100/600], Loss: 0.2816\n",
      "Epoch [40/100], Step [200/600], Loss: 0.1700\n",
      "Epoch [40/100], Step [300/600], Loss: 0.1997\n",
      "Epoch [40/100], Step [400/600], Loss: 0.2431\n",
      "Epoch [40/100], Step [500/600], Loss: 0.2214\n",
      "Epoch [40/100], Step [600/600], Loss: 0.1531\n",
      "Epoch [41/100], Step [100/600], Loss: 0.1722\n",
      "Epoch [41/100], Step [200/600], Loss: 0.0985\n",
      "Epoch [41/100], Step [300/600], Loss: 0.1689\n",
      "Epoch [41/100], Step [400/600], Loss: 0.1358\n",
      "Epoch [41/100], Step [500/600], Loss: 0.0956\n",
      "Epoch [41/100], Step [600/600], Loss: 0.2335\n",
      "Epoch [42/100], Step [100/600], Loss: 0.1703\n",
      "Epoch [42/100], Step [200/600], Loss: 0.2013\n",
      "Epoch [42/100], Step [300/600], Loss: 0.2184\n",
      "Epoch [42/100], Step [400/600], Loss: 0.1312\n",
      "Epoch [42/100], Step [500/600], Loss: 0.1928\n",
      "Epoch [42/100], Step [600/600], Loss: 0.2293\n",
      "Epoch [43/100], Step [100/600], Loss: 0.2259\n",
      "Epoch [43/100], Step [200/600], Loss: 0.1454\n",
      "Epoch [43/100], Step [300/600], Loss: 0.1567\n",
      "Epoch [43/100], Step [400/600], Loss: 0.1518\n",
      "Epoch [43/100], Step [500/600], Loss: 0.1904\n",
      "Epoch [43/100], Step [600/600], Loss: 0.3853\n",
      "Epoch [44/100], Step [100/600], Loss: 0.1061\n",
      "Epoch [44/100], Step [200/600], Loss: 0.1723\n",
      "Epoch [44/100], Step [300/600], Loss: 0.1669\n",
      "Epoch [44/100], Step [400/600], Loss: 0.2148\n",
      "Epoch [44/100], Step [500/600], Loss: 0.2059\n",
      "Epoch [44/100], Step [600/600], Loss: 0.1520\n",
      "Epoch [45/100], Step [100/600], Loss: 0.1377\n",
      "Epoch [45/100], Step [200/600], Loss: 0.1220\n",
      "Epoch [45/100], Step [300/600], Loss: 0.1320\n",
      "Epoch [45/100], Step [400/600], Loss: 0.1131\n",
      "Epoch [45/100], Step [500/600], Loss: 0.1764\n",
      "Epoch [45/100], Step [600/600], Loss: 0.1060\n",
      "Epoch [46/100], Step [100/600], Loss: 0.0921\n",
      "Epoch [46/100], Step [200/600], Loss: 0.0594\n",
      "Epoch [46/100], Step [300/600], Loss: 0.1966\n",
      "Epoch [46/100], Step [400/600], Loss: 0.1434\n",
      "Epoch [46/100], Step [500/600], Loss: 0.1482\n",
      "Epoch [46/100], Step [600/600], Loss: 0.1476\n",
      "Epoch [47/100], Step [100/600], Loss: 0.1818\n",
      "Epoch [47/100], Step [200/600], Loss: 0.1911\n",
      "Epoch [47/100], Step [300/600], Loss: 0.1235\n",
      "Epoch [47/100], Step [400/600], Loss: 0.1326\n",
      "Epoch [47/100], Step [500/600], Loss: 0.1378\n",
      "Epoch [47/100], Step [600/600], Loss: 0.1333\n",
      "Epoch [48/100], Step [100/600], Loss: 0.1542\n",
      "Epoch [48/100], Step [200/600], Loss: 0.2963\n",
      "Epoch [48/100], Step [300/600], Loss: 0.2300\n",
      "Epoch [48/100], Step [400/600], Loss: 0.2736\n",
      "Epoch [48/100], Step [500/600], Loss: 0.1572\n",
      "Epoch [48/100], Step [600/600], Loss: 0.1951\n",
      "Epoch [49/100], Step [100/600], Loss: 0.1500\n",
      "Epoch [49/100], Step [200/600], Loss: 0.1857\n",
      "Epoch [49/100], Step [300/600], Loss: 0.1240\n",
      "Epoch [49/100], Step [400/600], Loss: 0.1090\n",
      "Epoch [49/100], Step [500/600], Loss: 0.1647\n",
      "Epoch [49/100], Step [600/600], Loss: 0.1276\n",
      "Epoch [50/100], Step [100/600], Loss: 0.1114\n",
      "Epoch [50/100], Step [200/600], Loss: 0.1982\n",
      "Epoch [50/100], Step [300/600], Loss: 0.1483\n",
      "Epoch [50/100], Step [400/600], Loss: 0.1275\n",
      "Epoch [50/100], Step [500/600], Loss: 0.1093\n",
      "Epoch [50/100], Step [600/600], Loss: 0.2748\n",
      "Epoch [51/100], Step [100/600], Loss: 0.1349\n",
      "Epoch [51/100], Step [200/600], Loss: 0.1664\n",
      "Epoch [51/100], Step [300/600], Loss: 0.1481\n",
      "Epoch [51/100], Step [400/600], Loss: 0.1145\n",
      "Epoch [51/100], Step [500/600], Loss: 0.2000\n",
      "Epoch [51/100], Step [600/600], Loss: 0.1827\n",
      "Epoch [52/100], Step [100/600], Loss: 0.1499\n",
      "Epoch [52/100], Step [200/600], Loss: 0.0823\n",
      "Epoch [52/100], Step [300/600], Loss: 0.1953\n",
      "Epoch [52/100], Step [400/600], Loss: 0.1157\n",
      "Epoch [52/100], Step [500/600], Loss: 0.1424\n",
      "Epoch [52/100], Step [600/600], Loss: 0.0815\n",
      "Epoch [53/100], Step [100/600], Loss: 0.1443\n",
      "Epoch [53/100], Step [200/600], Loss: 0.1180\n",
      "Epoch [53/100], Step [300/600], Loss: 0.1863\n",
      "Epoch [53/100], Step [400/600], Loss: 0.1921\n",
      "Epoch [53/100], Step [500/600], Loss: 0.2320\n",
      "Epoch [53/100], Step [600/600], Loss: 0.1598\n",
      "Epoch [54/100], Step [100/600], Loss: 0.1919\n",
      "Epoch [54/100], Step [200/600], Loss: 0.0813\n",
      "Epoch [54/100], Step [300/600], Loss: 0.1424\n",
      "Epoch [54/100], Step [400/600], Loss: 0.1609\n",
      "Epoch [54/100], Step [500/600], Loss: 0.2296\n",
      "Epoch [54/100], Step [600/600], Loss: 0.1961\n",
      "Epoch [55/100], Step [100/600], Loss: 0.0912\n",
      "Epoch [55/100], Step [200/600], Loss: 0.1692\n",
      "Epoch [55/100], Step [300/600], Loss: 0.1486\n",
      "Epoch [55/100], Step [400/600], Loss: 0.1845\n",
      "Epoch [55/100], Step [500/600], Loss: 0.0918\n",
      "Epoch [55/100], Step [600/600], Loss: 0.1499\n",
      "Epoch [56/100], Step [100/600], Loss: 0.1883\n",
      "Epoch [56/100], Step [200/600], Loss: 0.1171\n",
      "Epoch [56/100], Step [300/600], Loss: 0.0976\n",
      "Epoch [56/100], Step [400/600], Loss: 0.1370\n",
      "Epoch [56/100], Step [500/600], Loss: 0.1119\n",
      "Epoch [56/100], Step [600/600], Loss: 0.0969\n",
      "Epoch [57/100], Step [100/600], Loss: 0.1382\n",
      "Epoch [57/100], Step [200/600], Loss: 0.1376\n",
      "Epoch [57/100], Step [300/600], Loss: 0.1785\n",
      "Epoch [57/100], Step [400/600], Loss: 0.1409\n",
      "Epoch [57/100], Step [500/600], Loss: 0.1396\n",
      "Epoch [57/100], Step [600/600], Loss: 0.0954\n",
      "Epoch [58/100], Step [100/600], Loss: 0.0940\n",
      "Epoch [58/100], Step [200/600], Loss: 0.1157\n",
      "Epoch [58/100], Step [300/600], Loss: 0.1281\n",
      "Epoch [58/100], Step [400/600], Loss: 0.1036\n",
      "Epoch [58/100], Step [500/600], Loss: 0.0696\n",
      "Epoch [58/100], Step [600/600], Loss: 0.1114\n",
      "Epoch [59/100], Step [100/600], Loss: 0.1917\n",
      "Epoch [59/100], Step [200/600], Loss: 0.1271\n",
      "Epoch [59/100], Step [300/600], Loss: 0.1791\n",
      "Epoch [59/100], Step [400/600], Loss: 0.1153\n",
      "Epoch [59/100], Step [500/600], Loss: 0.1789\n",
      "Epoch [59/100], Step [600/600], Loss: 0.2291\n",
      "Epoch [60/100], Step [100/600], Loss: 0.1513\n",
      "Epoch [60/100], Step [200/600], Loss: 0.1301\n",
      "Epoch [60/100], Step [300/600], Loss: 0.1196\n",
      "Epoch [60/100], Step [400/600], Loss: 0.0801\n",
      "Epoch [60/100], Step [500/600], Loss: 0.1831\n",
      "Epoch [60/100], Step [600/600], Loss: 0.1766\n",
      "Epoch [61/100], Step [100/600], Loss: 0.0939\n",
      "Epoch [61/100], Step [200/600], Loss: 0.0596\n",
      "Epoch [61/100], Step [300/600], Loss: 0.1299\n",
      "Epoch [61/100], Step [400/600], Loss: 0.0698\n",
      "Epoch [61/100], Step [500/600], Loss: 0.0944\n",
      "Epoch [61/100], Step [600/600], Loss: 0.1236\n",
      "Epoch [62/100], Step [100/600], Loss: 0.0909\n",
      "Epoch [62/100], Step [200/600], Loss: 0.1076\n",
      "Epoch [62/100], Step [300/600], Loss: 0.0835\n",
      "Epoch [62/100], Step [400/600], Loss: 0.0888\n",
      "Epoch [62/100], Step [500/600], Loss: 0.0930\n",
      "Epoch [62/100], Step [600/600], Loss: 0.2067\n",
      "Epoch [63/100], Step [100/600], Loss: 0.1729\n",
      "Epoch [63/100], Step [200/600], Loss: 0.1262\n",
      "Epoch [63/100], Step [300/600], Loss: 0.1293\n",
      "Epoch [63/100], Step [400/600], Loss: 0.1783\n",
      "Epoch [63/100], Step [500/600], Loss: 0.1212\n",
      "Epoch [63/100], Step [600/600], Loss: 0.0772\n",
      "Epoch [64/100], Step [100/600], Loss: 0.0910\n",
      "Epoch [64/100], Step [200/600], Loss: 0.1918\n",
      "Epoch [64/100], Step [300/600], Loss: 0.1001\n",
      "Epoch [64/100], Step [400/600], Loss: 0.1066\n",
      "Epoch [64/100], Step [500/600], Loss: 0.0982\n",
      "Epoch [64/100], Step [600/600], Loss: 0.2037\n",
      "Epoch [65/100], Step [100/600], Loss: 0.1467\n",
      "Epoch [65/100], Step [200/600], Loss: 0.1166\n",
      "Epoch [65/100], Step [300/600], Loss: 0.0749\n",
      "Epoch [65/100], Step [400/600], Loss: 0.1691\n",
      "Epoch [65/100], Step [500/600], Loss: 0.1650\n",
      "Epoch [65/100], Step [600/600], Loss: 0.1279\n",
      "Epoch [66/100], Step [100/600], Loss: 0.1285\n",
      "Epoch [66/100], Step [200/600], Loss: 0.0711\n",
      "Epoch [66/100], Step [300/600], Loss: 0.1069\n",
      "Epoch [66/100], Step [400/600], Loss: 0.1480\n",
      "Epoch [66/100], Step [500/600], Loss: 0.1343\n",
      "Epoch [66/100], Step [600/600], Loss: 0.0589\n",
      "Epoch [67/100], Step [100/600], Loss: 0.2045\n",
      "Epoch [67/100], Step [200/600], Loss: 0.1192\n",
      "Epoch [67/100], Step [300/600], Loss: 0.1951\n",
      "Epoch [67/100], Step [400/600], Loss: 0.0704\n",
      "Epoch [67/100], Step [500/600], Loss: 0.1084\n",
      "Epoch [67/100], Step [600/600], Loss: 0.2894\n",
      "Epoch [68/100], Step [100/600], Loss: 0.1178\n",
      "Epoch [68/100], Step [200/600], Loss: 0.0759\n",
      "Epoch [68/100], Step [300/600], Loss: 0.1907\n",
      "Epoch [68/100], Step [400/600], Loss: 0.1248\n",
      "Epoch [68/100], Step [500/600], Loss: 0.1262\n",
      "Epoch [68/100], Step [600/600], Loss: 0.1840\n",
      "Epoch [69/100], Step [100/600], Loss: 0.0693\n",
      "Epoch [69/100], Step [200/600], Loss: 0.1297\n",
      "Epoch [69/100], Step [300/600], Loss: 0.2567\n",
      "Epoch [69/100], Step [400/600], Loss: 0.0527\n",
      "Epoch [69/100], Step [500/600], Loss: 0.1214\n",
      "Epoch [69/100], Step [600/600], Loss: 0.0914\n",
      "Epoch [70/100], Step [100/600], Loss: 0.1320\n",
      "Epoch [70/100], Step [200/600], Loss: 0.1802\n",
      "Epoch [70/100], Step [300/600], Loss: 0.1084\n",
      "Epoch [70/100], Step [400/600], Loss: 0.0758\n",
      "Epoch [70/100], Step [500/600], Loss: 0.0723\n",
      "Epoch [70/100], Step [600/600], Loss: 0.0678\n",
      "Epoch [71/100], Step [100/600], Loss: 0.1479\n",
      "Epoch [71/100], Step [200/600], Loss: 0.1388\n",
      "Epoch [71/100], Step [300/600], Loss: 0.1306\n",
      "Epoch [71/100], Step [400/600], Loss: 0.1510\n",
      "Epoch [71/100], Step [500/600], Loss: 0.1146\n",
      "Epoch [71/100], Step [600/600], Loss: 0.1628\n",
      "Epoch [72/100], Step [100/600], Loss: 0.2016\n",
      "Epoch [72/100], Step [200/600], Loss: 0.1305\n",
      "Epoch [72/100], Step [300/600], Loss: 0.1232\n",
      "Epoch [72/100], Step [400/600], Loss: 0.1313\n",
      "Epoch [72/100], Step [500/600], Loss: 0.1808\n",
      "Epoch [72/100], Step [600/600], Loss: 0.1425\n",
      "Epoch [73/100], Step [100/600], Loss: 0.0635\n",
      "Epoch [73/100], Step [200/600], Loss: 0.1785\n",
      "Epoch [73/100], Step [300/600], Loss: 0.1127\n",
      "Epoch [73/100], Step [400/600], Loss: 0.1508\n",
      "Epoch [73/100], Step [500/600], Loss: 0.1539\n",
      "Epoch [73/100], Step [600/600], Loss: 0.2855\n",
      "Epoch [74/100], Step [100/600], Loss: 0.1275\n",
      "Epoch [74/100], Step [200/600], Loss: 0.0393\n",
      "Epoch [74/100], Step [300/600], Loss: 0.0791\n",
      "Epoch [74/100], Step [400/600], Loss: 0.2054\n",
      "Epoch [74/100], Step [500/600], Loss: 0.0978\n",
      "Epoch [74/100], Step [600/600], Loss: 0.1032\n",
      "Epoch [75/100], Step [100/600], Loss: 0.1174\n",
      "Epoch [75/100], Step [200/600], Loss: 0.0524\n",
      "Epoch [75/100], Step [300/600], Loss: 0.0998\n",
      "Epoch [75/100], Step [400/600], Loss: 0.0452\n",
      "Epoch [75/100], Step [500/600], Loss: 0.1658\n",
      "Epoch [75/100], Step [600/600], Loss: 0.1397\n",
      "Epoch [76/100], Step [100/600], Loss: 0.1251\n",
      "Epoch [76/100], Step [200/600], Loss: 0.0848\n",
      "Epoch [76/100], Step [300/600], Loss: 0.1082\n",
      "Epoch [76/100], Step [400/600], Loss: 0.1237\n",
      "Epoch [76/100], Step [500/600], Loss: 0.0656\n",
      "Epoch [76/100], Step [600/600], Loss: 0.1234\n",
      "Epoch [77/100], Step [100/600], Loss: 0.1168\n",
      "Epoch [77/100], Step [200/600], Loss: 0.0875\n",
      "Epoch [77/100], Step [300/600], Loss: 0.0892\n",
      "Epoch [77/100], Step [400/600], Loss: 0.1052\n",
      "Epoch [77/100], Step [500/600], Loss: 0.0974\n",
      "Epoch [77/100], Step [600/600], Loss: 0.2568\n",
      "Epoch [78/100], Step [100/600], Loss: 0.0694\n",
      "Epoch [78/100], Step [200/600], Loss: 0.1715\n",
      "Epoch [78/100], Step [300/600], Loss: 0.1113\n",
      "Epoch [78/100], Step [400/600], Loss: 0.1378\n",
      "Epoch [78/100], Step [500/600], Loss: 0.0938\n",
      "Epoch [78/100], Step [600/600], Loss: 0.1125\n",
      "Epoch [79/100], Step [100/600], Loss: 0.1790\n",
      "Epoch [79/100], Step [200/600], Loss: 0.1017\n",
      "Epoch [79/100], Step [300/600], Loss: 0.0308\n",
      "Epoch [79/100], Step [400/600], Loss: 0.1501\n",
      "Epoch [79/100], Step [500/600], Loss: 0.2444\n",
      "Epoch [79/100], Step [600/600], Loss: 0.1124\n",
      "Epoch [80/100], Step [100/600], Loss: 0.2621\n",
      "Epoch [80/100], Step [200/600], Loss: 0.1046\n",
      "Epoch [80/100], Step [300/600], Loss: 0.1094\n",
      "Epoch [80/100], Step [400/600], Loss: 0.0946\n",
      "Epoch [80/100], Step [500/600], Loss: 0.0675\n",
      "Epoch [80/100], Step [600/600], Loss: 0.0785\n",
      "Epoch [81/100], Step [100/600], Loss: 0.0871\n",
      "Epoch [81/100], Step [200/600], Loss: 0.0682\n",
      "Epoch [81/100], Step [300/600], Loss: 0.0956\n",
      "Epoch [81/100], Step [400/600], Loss: 0.0578\n",
      "Epoch [81/100], Step [500/600], Loss: 0.1453\n",
      "Epoch [81/100], Step [600/600], Loss: 0.1544\n",
      "Epoch [82/100], Step [100/600], Loss: 0.1396\n",
      "Epoch [82/100], Step [200/600], Loss: 0.0256\n",
      "Epoch [82/100], Step [300/600], Loss: 0.1330\n",
      "Epoch [82/100], Step [400/600], Loss: 0.1026\n",
      "Epoch [82/100], Step [500/600], Loss: 0.1119\n",
      "Epoch [82/100], Step [600/600], Loss: 0.1007\n",
      "Epoch [83/100], Step [100/600], Loss: 0.1433\n",
      "Epoch [83/100], Step [200/600], Loss: 0.1131\n",
      "Epoch [83/100], Step [300/600], Loss: 0.1335\n",
      "Epoch [83/100], Step [400/600], Loss: 0.1438\n",
      "Epoch [83/100], Step [500/600], Loss: 0.1012\n",
      "Epoch [83/100], Step [600/600], Loss: 0.1193\n",
      "Epoch [84/100], Step [100/600], Loss: 0.1086\n",
      "Epoch [84/100], Step [200/600], Loss: 0.1031\n",
      "Epoch [84/100], Step [300/600], Loss: 0.0420\n",
      "Epoch [84/100], Step [400/600], Loss: 0.1165\n",
      "Epoch [84/100], Step [500/600], Loss: 0.1489\n",
      "Epoch [84/100], Step [600/600], Loss: 0.0403\n",
      "Epoch [85/100], Step [100/600], Loss: 0.0730\n",
      "Epoch [85/100], Step [200/600], Loss: 0.0617\n",
      "Epoch [85/100], Step [300/600], Loss: 0.0621\n",
      "Epoch [85/100], Step [400/600], Loss: 0.0964\n",
      "Epoch [85/100], Step [500/600], Loss: 0.0500\n",
      "Epoch [85/100], Step [600/600], Loss: 0.0634\n",
      "Epoch [86/100], Step [100/600], Loss: 0.2027\n",
      "Epoch [86/100], Step [200/600], Loss: 0.1205\n",
      "Epoch [86/100], Step [300/600], Loss: 0.1457\n",
      "Epoch [86/100], Step [400/600], Loss: 0.0990\n",
      "Epoch [86/100], Step [500/600], Loss: 0.1971\n",
      "Epoch [86/100], Step [600/600], Loss: 0.2109\n",
      "Epoch [87/100], Step [100/600], Loss: 0.0769\n",
      "Epoch [87/100], Step [200/600], Loss: 0.1740\n",
      "Epoch [87/100], Step [300/600], Loss: 0.1618\n",
      "Epoch [87/100], Step [400/600], Loss: 0.2014\n",
      "Epoch [87/100], Step [500/600], Loss: 0.1793\n",
      "Epoch [87/100], Step [600/600], Loss: 0.0420\n",
      "Epoch [88/100], Step [100/600], Loss: 0.0811\n",
      "Epoch [88/100], Step [200/600], Loss: 0.1888\n",
      "Epoch [88/100], Step [300/600], Loss: 0.1509\n",
      "Epoch [88/100], Step [400/600], Loss: 0.1150\n",
      "Epoch [88/100], Step [500/600], Loss: 0.0751\n",
      "Epoch [88/100], Step [600/600], Loss: 0.0892\n",
      "Epoch [89/100], Step [100/600], Loss: 0.0544\n",
      "Epoch [89/100], Step [200/600], Loss: 0.0713\n",
      "Epoch [89/100], Step [300/600], Loss: 0.0928\n",
      "Epoch [89/100], Step [400/600], Loss: 0.1056\n",
      "Epoch [89/100], Step [500/600], Loss: 0.0959\n",
      "Epoch [89/100], Step [600/600], Loss: 0.0835\n",
      "Epoch [90/100], Step [100/600], Loss: 0.1124\n",
      "Epoch [90/100], Step [200/600], Loss: 0.2438\n",
      "Epoch [90/100], Step [300/600], Loss: 0.0633\n",
      "Epoch [90/100], Step [400/600], Loss: 0.1166\n",
      "Epoch [90/100], Step [500/600], Loss: 0.0993\n",
      "Epoch [90/100], Step [600/600], Loss: 0.0939\n",
      "Epoch [91/100], Step [100/600], Loss: 0.0961\n",
      "Epoch [91/100], Step [200/600], Loss: 0.1469\n",
      "Epoch [91/100], Step [300/600], Loss: 0.0934\n",
      "Epoch [91/100], Step [400/600], Loss: 0.1683\n",
      "Epoch [91/100], Step [500/600], Loss: 0.0775\n",
      "Epoch [91/100], Step [600/600], Loss: 0.0996\n",
      "Epoch [92/100], Step [100/600], Loss: 0.1194\n",
      "Epoch [92/100], Step [200/600], Loss: 0.1007\n",
      "Epoch [92/100], Step [300/600], Loss: 0.0363\n",
      "Epoch [92/100], Step [400/600], Loss: 0.1280\n",
      "Epoch [92/100], Step [500/600], Loss: 0.0972\n",
      "Epoch [92/100], Step [600/600], Loss: 0.1050\n",
      "Epoch [93/100], Step [100/600], Loss: 0.2738\n",
      "Epoch [93/100], Step [200/600], Loss: 0.1837\n",
      "Epoch [93/100], Step [300/600], Loss: 0.0534\n",
      "Epoch [93/100], Step [400/600], Loss: 0.1582\n",
      "Epoch [93/100], Step [500/600], Loss: 0.1270\n",
      "Epoch [93/100], Step [600/600], Loss: 0.0855\n",
      "Epoch [94/100], Step [100/600], Loss: 0.0747\n",
      "Epoch [94/100], Step [200/600], Loss: 0.0831\n",
      "Epoch [94/100], Step [300/600], Loss: 0.1458\n",
      "Epoch [94/100], Step [400/600], Loss: 0.1414\n",
      "Epoch [94/100], Step [500/600], Loss: 0.1181\n",
      "Epoch [94/100], Step [600/600], Loss: 0.0606\n",
      "Epoch [95/100], Step [100/600], Loss: 0.1425\n",
      "Epoch [95/100], Step [200/600], Loss: 0.0543\n",
      "Epoch [95/100], Step [300/600], Loss: 0.1065\n",
      "Epoch [95/100], Step [400/600], Loss: 0.0708\n",
      "Epoch [95/100], Step [500/600], Loss: 0.1202\n",
      "Epoch [95/100], Step [600/600], Loss: 0.0540\n",
      "Epoch [96/100], Step [100/600], Loss: 0.1198\n",
      "Epoch [96/100], Step [200/600], Loss: 0.0416\n",
      "Epoch [96/100], Step [300/600], Loss: 0.0996\n",
      "Epoch [96/100], Step [400/600], Loss: 0.1089\n",
      "Epoch [96/100], Step [500/600], Loss: 0.1529\n",
      "Epoch [96/100], Step [600/600], Loss: 0.0762\n",
      "Epoch [97/100], Step [100/600], Loss: 0.1496\n",
      "Epoch [97/100], Step [200/600], Loss: 0.0916\n",
      "Epoch [97/100], Step [300/600], Loss: 0.2032\n",
      "Epoch [97/100], Step [400/600], Loss: 0.0988\n",
      "Epoch [97/100], Step [500/600], Loss: 0.1331\n",
      "Epoch [97/100], Step [600/600], Loss: 0.1706\n",
      "Epoch [98/100], Step [100/600], Loss: 0.0618\n",
      "Epoch [98/100], Step [200/600], Loss: 0.0810\n",
      "Epoch [98/100], Step [300/600], Loss: 0.0657\n",
      "Epoch [98/100], Step [400/600], Loss: 0.0631\n",
      "Epoch [98/100], Step [500/600], Loss: 0.0860\n",
      "Epoch [98/100], Step [600/600], Loss: 0.0755\n",
      "Epoch [99/100], Step [100/600], Loss: 0.0486\n",
      "Epoch [99/100], Step [200/600], Loss: 0.1550\n",
      "Epoch [99/100], Step [300/600], Loss: 0.1550\n",
      "Epoch [99/100], Step [400/600], Loss: 0.0669\n",
      "Epoch [99/100], Step [500/600], Loss: 0.0746\n",
      "Epoch [99/100], Step [600/600], Loss: 0.2443\n",
      "Epoch [100/100], Step [100/600], Loss: 0.1782\n",
      "Epoch [100/100], Step [200/600], Loss: 0.1177\n",
      "Epoch [100/100], Step [300/600], Loss: 0.0656\n",
      "Epoch [100/100], Step [400/600], Loss: 0.1241\n",
      "Epoch [100/100], Step [500/600], Loss: 0.0970\n",
      "Epoch [100/100], Step [600/600], Loss: 0.0904\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracies1, size1 = runModel(1)\n",
    "plt.plot(size1, accuracies1)\n",
    "# accuracies2, size2 = runModel(2)\n",
    "# plt.plot(size2, accuracies2)\n",
    "# accuracies5, size5 = runModel(5)\n",
    "# plt.plot(size5, accuracies5)\n",
    "# accuracies100, size100 = runModel(10)\n",
    "# plt.plot(size100, accuracies100)\n",
    "plt.xlabel('Number of Hidden Neurons')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Numerical Simulation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(accuracies1)\n",
    "# size1 = []\n",
    "# for i in range(10):\n",
    "#     size1.append(50)\n",
    "# for i in range(10):\n",
    "#     size1.append(100)\n",
    "# for i in range(10):\n",
    "#     size1.append(150)\n",
    "# for i in range(10):\n",
    "#     size1.append(200)\n",
    "# for i in range(10):\n",
    "#     size1.append(250)\n",
    "# for i in range(10):\n",
    "#     size1.append(300)\n",
    "# for i in range(10):\n",
    "#     size1.append(350)\n",
    "# for i in range(10):\n",
    "#     size1.append(400)\n",
    "\n",
    "# plt.plot(size1, accuracies1)\n",
    "# plt.xlabel('Number of Hidden Neurons')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.title('Numerical Simulation')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96.31037139892578]\n"
     ]
    }
   ],
   "source": [
    "print(accuracies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Pytorch MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
